import base64
import os
import math
import pickle
import jieba
import json
import docx
from unstructured.partition.pdf import partition_pdf
from unstructured.documents.elements import Table, Image as UnstructuredImage
# å¼•å…¥å¿…è¦çš„ LangChain æ¶ˆæ¯ç±»
from langchain_core.messages import HumanMessage, SystemMessage
import ast  # ç”¨äºè§£æå…ƒæ•°æ®ä¸­çš„åˆ—è¡¨å­—ç¬¦ä¸²
from dataclasses import dataclass, asdict
import logging
logging.getLogger("pdfminer").setLevel(logging.ERROR)
logging.getLogger("langdetect").setLevel(logging.ERROR)
from typing import List, Dict, Any, Optional
from pathlib import Path
from langchain_community.document_loaders import TextLoader, PyPDFLoader, UnstructuredWordDocumentLoader
from langchain_community.retrievers import BM25Retriever
from langchain_classic.retrievers import EnsembleRetriever
from langchain_chroma import Chroma
import hashlib
from langchain_huggingface import HuggingFaceEmbeddings
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from sentence_transformers import CrossEncoder
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_community.chat_message_histories import SQLChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.callbacks import CallbackManagerForRetrieverRun
import numpy as np
import shutil
from collections import defaultdict
from langchain_community.vectorstores.utils import filter_complex_metadata
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')  # å¢åŠ æ—¶é—´æˆ³
logger = logging.getLogger(__name__)
import pytesseract
tesseract_dir = r"C:\Program Files\Tesseract-OCR"
tessdata_dir = os.path.join(tesseract_dir, "tessdata")
pytesseract.pytesseract.tesseract_cmd = os.path.join(tesseract_dir, "tesseract.exe")
# å‡è®¾ä½ è§£å‹åˆ°äº†è¿™ä¸ªä½ç½®ï¼Œè¯·ä¿®æ”¹ä¸ºå®é™…è·¯å¾„\
os.environ["PATH"] += os.pathsep + tesseract_dir
os.environ["TESSDATA_PREFIX"] = tessdata_dir

class UnstructuredPDFParser:

    def __init__(self, base_path: Path):
        # å›¾ç‰‡ä¿å­˜çš„åŸºç¡€è·¯å¾„
        self.output_dir = base_path / "assets"
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def extract(self, file_path: str) -> List[Document]:
        file_path_obj = Path(file_path)
        logger.info(f"æ­£åœ¨ä½¿ç”¨ Unstructured è§£æ PDF (hi_res æ¨¡å¼): {file_path_obj.name} ... è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´")

        try:
            # ã€ä¿®æ”¹ç‚¹ 1ã€‘å°† "Table" åŠ å…¥å›¾ç‰‡æå–åˆ—è¡¨
            # infer_table_structure=False (æ—¢ç„¶å½“å›¾ç‰‡çœ‹ï¼Œå°±ä¸éœ€è¦æµªè´¹æ—¶é—´è§£æ HTML ç»“æ„äº†ï¼Œé™¤ééœ€è¦ OCR æ–‡æœ¬è¾…åŠ©æ£€ç´¢)
            # å»ºè®®ä¿ç•™ infer_table_structure=True ä»¥è·å¾—æ›´å¥½çš„ OCR æ–‡æœ¬ç”¨äº Search ç´¢å¼•ï¼Œä½†å¿½ç•¥å…¶ HTML è¾“å‡º
            elements = partition_pdf(
                filename=file_path,
                strategy="hi_res",
                extract_images_in_pdf=True,
                extract_image_block_types=["Image", "Table"],  # ã€å…³é”®ã€‘æŠŠè¡¨æ ¼å½“å›¾ç‰‡åˆ‡å‡ºæ¥
                extract_image_block_output_dir=str(self.output_dir),
                infer_table_structure=True,  # ä¾ç„¶å¼€å¯ï¼Œä¸ºäº†è·å– element.text ç”¨äºå‘é‡æ£€ç´¢
                chunking_strategy="by_title",  # ä½¿ç”¨ Unstructured è‡ªå¸¦çš„è¯­ä¹‰åˆ†å—ä½œä¸ºç¬¬ä¸€å±‚åˆ‡å‰²
                max_characters=5000,  # é™åˆ¶åˆå§‹å—å¤§å°
                new_after_n_chars=300,
                combine_text_under_n_chars=1000,
                languages=["eng", "chi_sim"],
            )
        except Exception as e:
            logger.error(f"Unstructured è§£æå¤±è´¥: {e}", exc_info=True)
            return []

        documents = []
        for el in elements:
            # è·å–åŸºæœ¬å…ƒæ•°æ®
            page_num = el.metadata.page_number - 1 if el.metadata.page_number else 0
            file_name = file_path_obj.name

            # å‡†å¤‡ Document çš„ metadata
            metadata = {
                "source": str(file_path_obj),
                "file_name": file_name,
                "page": page_num,
                "image_paths": []  # åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
            }

            # ã€ä¿®æ”¹ç‚¹ 2ã€‘å¤„ç†è¡¨æ ¼å’Œå›¾ç‰‡å…ƒç´ 
            if isinstance(el, Table) or isinstance(el, UnstructuredImage):
                # å°è¯•è·å–å›¾ç‰‡è·¯å¾„
                if el.metadata.image_path:
                    img_path = str(Path(el.metadata.image_path).resolve())
                    # å°†å›¾ç‰‡è·¯å¾„å­˜å…¥ list (å…¼å®¹åŸæœ‰é€»è¾‘)
                    metadata["image_paths"].append(img_path)
                    metadata["is_image_asset"] = True  # æ ‡è®°è¿™æ˜¯ä¸ªè§†è§‰èµ„äº§
                # å†³å®š page_content
                # å¯¹äºè¡¨æ ¼ï¼Œæˆ‘ä»¬ä½¿ç”¨ OCR è¯†åˆ«åˆ°çš„æ–‡æœ¬ä½œä¸º contentï¼Œä»¥ä¾¿ BM25/Vector èƒ½æ£€ç´¢åˆ°å®ƒ
                # æ£€ç´¢åˆ°åï¼ŒLLM ä¼šçœ‹ metadata é‡Œçš„å›¾ç‰‡
                if el.text and el.text.strip():
                    content = el.text
                else:
                    # å¦‚æœ OCR æ²¡è¯†åˆ«å‡ºå­—ï¼Œç»™ä¸ªå ä½ç¬¦ï¼Œé˜²æ­¢ç©ºå†…å®¹è¢«ä¸¢å¼ƒ
                    content = f"[Visual Data] Type: {type(el).__name__}"
                # æ ‡è®°ç±»å‹ï¼Œé˜²æ­¢è¢« Splitter åˆ‡ç¢
                metadata["do_not_split"] = True
                documents.append(Document(page_content=content, metadata=metadata))
            # ã€ä¿®æ”¹ç‚¹ 3ã€‘å¤„ç†æ™®é€šæ–‡æœ¬
            else:
                if el.text and el.text.strip():
                    # æ™®é€šæ–‡æœ¬ï¼Œæ ‡è®°ä¸º Falseï¼Œå…è®¸åç»­è¢« RecursiveSplitter è¿›ä¸€æ­¥å¤„ç†
                    metadata["is_image_asset"] = False
                    metadata["do_not_split"] = False
                    documents.append(Document(page_content=el.text, metadata=metadata))

        logger.info(f"Unstructured è§£æå®Œæˆï¼Œç”Ÿæˆ {len(documents)} ä¸ªåŸå§‹æ–‡æ¡£ç‰‡æ®µã€‚")
        return documents


class ChineseBM25Retriever(BM25Retriever):
    """é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–çš„ BM25 æ£€ç´¢å™¨"""

    @classmethod
    def from_documents(cls, documents, **kwargs):
        # å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œä¸­æ–‡åˆ†è¯é¢„å¤„ç†
        processed_texts = []
        for doc in documents:
            # ä½¿ç”¨ jieba åˆ†è¯
            words = jieba.lcut(doc.page_content)
            processed_texts.append(" ".join(words))

        # åˆ›å»ºä¸´æ—¶æ–‡æ¡£ç”¨äº BM25
        processed_docs = [
            Document(page_content=text, metadata=doc.metadata)
            for text, doc in zip(processed_texts, documents)
        ]
        return super().from_documents(processed_docs, **kwargs)

    def _get_relevant_documents(
        self,
        query: str,
        *,
        run_manager: CallbackManagerForRetrieverRun = None
    ) -> List[Document]:
        # å¯¹æŸ¥è¯¢ä¹Ÿè¿›è¡Œåˆ†è¯
        query_words = " ".join(jieba.lcut(query))
        # å°† run_manager ä¼ é€’ç»™çˆ¶ç±»
        return super()._get_relevant_documents(query_words, run_manager=run_manager)


# ã€ä¿®æ”¹ç‚¹ 1ã€‘æ·»åŠ  @dataclass è£…é¥°å™¨
@dataclass
class ChunkData:
    """å­˜å‚¨å•ä¸ª chunk çš„æ•°æ®ç»“æ„"""
    content: str
    metadata: Dict[str, Any]
    chunk_id: str


class ChunkStore:
    """
    Chunks æŒä¹…åŒ–å­˜å‚¨ç®¡ç†å™¨
    æ”¯æŒä¸¤ç§å­˜å‚¨æ ¼å¼ï¼š
    1. JSONï¼ˆå¯è¯»æ€§å¥½ï¼Œé€‚åˆè°ƒè¯•ï¼‰
    2. Pickleï¼ˆæ€§èƒ½æ›´å¥½ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼‰
    """

    def __init__(self, store_path: Path, use_pickle: bool = True):
        self.store_path = store_path
        self.use_pickle = use_pickle
        self.store_path.mkdir(parents=True, exist_ok=True)

        # å­˜å‚¨æ–‡ä»¶è·¯å¾„
        self.chunks_file = self.store_path / ("chunks.pkl" if use_pickle else "chunks.json")
        self.index_file = self.store_path / "chunk_index.json"

        # å†…å­˜ç¼“å­˜
        self._chunks: Dict[str, ChunkData] = {}  # chunk_id -> ChunkData
        self._file_chunks: Dict[str, List[str]] = {}  # file_path -> [chunk_ids]
        # ã€æ–°å¢ã€‘è¾…åŠ©å­—å…¸ï¼Œç”¨äºä¸åŒºåˆ†å¤§å°å†™çš„è·¯å¾„æŸ¥æ‰¾
        self._file_chunks_lower: Dict[str, str] = {}

        self._load()

    def _load(self):
        """ä»ç£ç›˜åŠ è½½å·²å­˜å‚¨çš„ chunks"""
        try:
            # åŠ è½½ç´¢å¼•
            if self.index_file.exists():
                with open(self.index_file, 'r', encoding='utf-8') as f:
                    self._file_chunks = json.load(f)
                # ã€æ–°å¢ã€‘æ„å»ºå°å†™æ˜ å°„è¡¨
                self._file_chunks_lower = {k.lower(): k for k in self._file_chunks.keys()}
                logger.info(f"åŠ è½½äº† {len(self._file_chunks)} ä¸ªæ–‡ä»¶çš„ chunk ç´¢å¼•")

            # åŠ è½½ chunks æ•°æ®
            if self.chunks_file.exists():
                if self.use_pickle:
                    with open(self.chunks_file, 'rb') as f:
                        self._chunks = pickle.load(f)
                else:
                    with open(self.chunks_file, 'r', encoding='utf-8') as f:
                        raw_data = json.load(f)
                        self._chunks = {
                            k: ChunkData(**v) for k, v in raw_data.items()
                        }
                logger.info(f"åŠ è½½äº† {len(self._chunks)} ä¸ª chunks")
        except Exception as e:
            logger.error(f"åŠ è½½ chunk store å¤±è´¥: {e}")
            self._chunks = {}
            self._file_chunks = {}
            self._file_chunks_lower = {}

    def _save(self):
        """ä¿å­˜ chunks åˆ°ç£ç›˜ (å¢åŠ åŸå­å†™å…¥ä¿æŠ¤)"""
        try:
            # ä¿å­˜ç´¢å¼•
            with open(self.index_file, 'w', encoding='utf-8') as f:
                json.dump(self._file_chunks, f, ensure_ascii=False, indent=2)

            # ä¿å­˜ chunks æ•°æ®
            if self.use_pickle:
                temp_file = self.chunks_file.with_suffix('.tmp')
                with open(temp_file, 'wb') as f:
                    pickle.dump(self._chunks, f)
                if self.chunks_file.exists():
                    os.remove(self.chunks_file)
                os.rename(temp_file, self.chunks_file)
            else:
                with open(self.chunks_file, 'w', encoding='utf-8') as f:
                    json.dump(
                        {k: asdict(v) for k, v in self._chunks.items()},
                        f, ensure_ascii=False, indent=2
                    )
            logger.info(f"ä¿å­˜äº† {len(self._chunks)} ä¸ª chunks åˆ°ç£ç›˜")
        except Exception as e:
            logger.error(f"ä¿å­˜ chunk store å¤±è´¥: {e}")

    def add_chunks(self, file_path: str, chunks: List[Document]) -> List[str]:
        """
        æ·»åŠ æ–‡ä»¶çš„ chunks
        è¿”å› chunk_ids åˆ—è¡¨
        """
        chunk_ids = []
        for chunk in chunks:
            chunk_id = chunk.metadata.get('chunk_id')
            if not chunk_id:
                # ç”Ÿæˆ chunk_id
                content_hash = hashlib.sha256(chunk.page_content.encode()).hexdigest()[:16]
                chunk_id = f"{hashlib.md5(file_path.encode()).hexdigest()[:8]}_{content_hash}"

            chunk_data = ChunkData(
                content=chunk.page_content,
                metadata=chunk.metadata,
                chunk_id=chunk_id
            )
            self._chunks[chunk_id] = chunk_data
            chunk_ids.append(chunk_id)

        self._file_chunks[file_path] = chunk_ids
        # ã€æ–°å¢ã€‘æ›´æ–°å°å†™æ˜ å°„
        self._file_chunks_lower[file_path.lower()] = file_path
        self._save()
        return chunk_ids

    def get_chunks_for_file(self, file_path: str) -> List[Document]:
        """è·å–æŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰ chunks (æ”¯æŒå¤§å°å†™æ¨¡ç³ŠåŒ¹é…)"""
        # 1. å°è¯•ç²¾ç¡®åŒ¹é…
        chunk_ids = self._file_chunks.get(file_path)

        # 2. å¦‚æœå¤±è´¥ï¼Œå°è¯•å°å†™æ¨¡ç³ŠåŒ¹é… (è§£å†³ Windows è·¯å¾„ä¸ä¸€è‡´é—®é¢˜)
        if chunk_ids is None:
            real_key = self._file_chunks_lower.get(file_path.lower())
            if real_key:
                chunk_ids = self._file_chunks.get(real_key, [])
            else:
                chunk_ids = []

        documents = []
        for chunk_id in chunk_ids:
            if chunk_id in self._chunks:
                chunk_data = self._chunks[chunk_id]
                documents.append(Document(
                    page_content=chunk_data.content,
                    metadata=chunk_data.metadata
                ))
        return documents

    def has_file(self, file_path: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜å‚¨ (æ”¯æŒæ¨¡ç³ŠåŒ¹é…)"""
        if file_path in self._file_chunks:
            return True
        return file_path.lower() in self._file_chunks_lower

    def delete_file(self, file_path: str) -> List[str]:
        """åˆ é™¤æ–‡ä»¶çš„æ‰€æœ‰ chunksï¼Œè¿”å›è¢«åˆ é™¤çš„ chunk_ids"""
        # å°è¯•è·å–çœŸå® Key
        real_key = file_path
        if real_key not in self._file_chunks:
            real_key = self._file_chunks_lower.get(file_path.lower())

        if not real_key:
            return []

        chunk_ids = self._file_chunks.pop(real_key, [])
        # æ¸…ç†è¾…åŠ©æ˜ å°„
        if real_key.lower() in self._file_chunks_lower:
            del self._file_chunks_lower[real_key.lower()]

        for chunk_id in chunk_ids:
            self._chunks.pop(chunk_id, None)
        self._save()
        return chunk_ids

    # ... å…¶ä»–æ–¹æ³• (get_all_chunks, get_chunk_ids_for_file, get_stats) ä¿æŒä¸å˜ ...
    def get_all_chunks(self) -> List[Document]:
        return [Document(page_content=cd.content, metadata=cd.metadata) for cd in self._chunks.values()]

    def get_chunk_ids_for_file(self, file_path: str) -> List[str]:
        return self._file_chunks.get(file_path, [])

    def get_stats(self) -> Dict[str, int]:
        return {
            "total_files": len(self._file_chunks),
            "total_chunks": len(self._chunks),
            "store_size_bytes": self.chunks_file.stat().st_size if self.chunks_file.exists() else 0
        }

class MultiDocumentKnowledgeBase:
    def __init__(self, kb_path: str,openai_api_key: str,llm_instance=None):
        self.kb_path = Path(kb_path).resolve()
        self.kb_path.mkdir(parents=True, exist_ok=True)
        self._openai_api_key = openai_api_key

        self.collection_name = "multi_doc_kb_collection_optimized"
        self.chroma_persist_path = str(self.kb_path / "chroma_db")
        Path(self.chroma_persist_path).mkdir(parents=True, exist_ok=True)
        logger.info(f"Chroma å­˜å‚¨è·¯å¾„: {self.chroma_persist_path}")

        self.chat_history_db_path = str(self.kb_path / "chat_histories.db")
        Path(self.chat_history_db_path).parent.mkdir(parents=True, exist_ok=True)
        logger.info(f"èŠå¤©å†å²æ•°æ®åº“è·¯å¾„: {self.chat_history_db_path}")

        #logger.info("æ­£åœ¨åŠ è½½ HuggingFace åµŒå…¥æ¨¡å‹:my_sentence_transformer_model")
        '''self.embeddings = HuggingFaceEmbeddings(
            model_name="output/my_sentence_transformer_model",  # Change this to your local model directory
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )'''
        embedding_model_name = "./models/Qwen3-Embedding-0.6B"
        logger.info(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹: {embedding_model_name}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model_name,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        logger.info("HuggingFace åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆã€‚")

        reranker_model_name = "./models/bge-reranker-large"
        # æˆ–è€…ç›´æ¥å†™ï¼šreranker_model_name = "./models/bge-reranker-large"
        logger.info(f"æ­£åœ¨ä»æœ¬åœ°åŠ è½½é‡æ–°æ’åæ¨¡å‹: {reranker_model_name}...")
        # CrossEncoder æ”¯æŒç›´æ¥ä¼ å…¥æœ¬åœ°è·¯å¾„
        self.reranker = CrossEncoder(reranker_model_name, device='cpu')
        logger.info("é‡æ–°æ’åæ¨¡å‹åŠ è½½å®Œæˆã€‚")


        if llm_instance:
            self.llm = llm_instance
        else:
                # é»˜è®¤å›é€€é€»è¾‘ï¼Œæˆ–è€…è¿æ¥æ‚¨çš„å¾®è°ƒæ¨¡å‹ endpoint
            self.llm = ChatOpenAI(
                openai_api_key=self._openai_api_key,  # æœ¬åœ°æ¨¡å‹é€šå¸¸ä¸éœ€è¦çœŸå® Key
                openai_api_base="https://api.zchat.tech/v1",  # æ‚¨çš„å¾®è°ƒæ¨¡å‹æœåŠ¡åœ°å€
                model_name="gpt-5-1",  # æ‚¨çš„æ¨¡å‹åç§°
                temperature=0.1  # ä¸“ä¸šé¢†åŸŸå»ºè®®ä½æ¸©åº¦
                )

        logger.info(f"æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {embedding_model_name} ä»¥å®ç° Token ç²¾ç¡®åˆ‡å‰²...")
        try:
            # åŠ è½½ä¸ Embedding æ¨¡å‹åŒ¹é…çš„åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(embedding_model_name, trust_remote_code=True)

            # ä½¿ç”¨ from_huggingface_tokenizer åˆ›å»ºåˆ†å‰²å™¨
            # chunk_size ç°åœ¨ä»£è¡¨ Token æ•°é‡ï¼Œè€Œä¸æ˜¯å­—ç¬¦æ•°
            # 512 token å¤§çº¦ç­‰äº 800-1000 ä¸ªæ±‰å­—ï¼Œé€‚åˆå¤§å¤šæ•° Embedding æ¨¡å‹
            self.text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(
                tokenizer=self.tokenizer,
                chunk_size=400,  # é™åˆ¶æ¯ä¸ªå— 500 tokens
                chunk_overlap=100,  # é‡å  100 tokens
                separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ".", " ", ""],  # ä¼˜å…ˆæŒ‰æ®µè½å’Œå¥å­åˆ‡
                strip_whitespace=True
            )
            logger.info(
                "æ–‡æœ¬åˆ†å‰²å™¨å·²æ›´æ–°ä¸º Token æ„ŸçŸ¥æ¨¡å¼ (RecursiveCharacterTextSplitter.from_huggingface_tokenizer)ã€‚")
        except Exception as e:
            logger.error(f"åŠ è½½åˆ†è¯å™¨å¤±è´¥ï¼Œå›é€€åˆ°å­—ç¬¦é•¿åº¦åˆ‡å‰²: {e}")
            # å›é€€é€»è¾‘
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=800,
                chunk_overlap=100,
                length_function=len
            )

        logger.info("æ–‡æœ¬åˆ†å‰²å™¨å·²åˆå§‹åŒ–ä¸º RecursiveCharacterTextSplitterã€‚")

        self.vectorstore: Optional[Chroma] = None
        self.documents: List[Document] = []
        self.doc_metadata: Dict[str, Any] = {}
        self._file_hashes: Dict[str, str] = {}

        self._bm25_retriever: Optional[BM25Retriever] = None
        self._chroma_retriever: Optional[Any] = None
        self.ensemble_retriever: Optional[EnsembleRetriever] = None

        logger.info("èŠå¤©å†å²å°†é€šè¿‡ SQLChatMessageHistory ç®¡ç†ã€‚")

        # ã€ä¿®æ”¹ç‚¹ 2ã€‘å¿…é¡»å…ˆåˆå§‹åŒ– chunk_storeï¼Œå› ä¸º _load_knowledge_base ä¼šç”¨åˆ°å®ƒ
        self.chunk_store = ChunkStore(
            self.kb_path / "chunk_store",
            use_pickle=False  # ä½¿ç”¨ pickle è·å¾—æ›´å¥½æ€§èƒ½
        )
        # ã€æ–°å¢ã€‘ åˆå§‹åŒ–é«˜çº§ PDF è§£æå™¨
        self.pdf_parser = UnstructuredPDFParser(self.kb_path)
        # åŠ è½½çŸ¥è¯†åº“å¿…é¡»åœ¨ chunk_store åˆå§‹åŒ–ä¹‹å
        self._load_knowledge_base()

    def parse_local_file(self, file_path: str) -> str:
        """
        ç›´æ¥è§£ææœ¬åœ°æ–‡ä»¶å†…å®¹è¿”å›å­—ç¬¦ä¸²ï¼Œä¸è¿›è¡Œå‘é‡å­˜å‚¨ã€‚
        ç”¨äºæå–å‚è€ƒç­”æ¡ˆã€ä¸´æ—¶ä¸Šä¸‹æ–‡ç­‰ã€‚
        """
        file_path_obj = Path(file_path).resolve()
        if not file_path_obj.exists():
            logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}")
            return ""

        logger.info(f"æ­£åœ¨è§£æä¸´æ—¶å‚è€ƒæ–‡ä»¶: {file_path_obj.name}")
        content = ""
        try:
            # æ ¹æ®åç¼€é€‰æ‹©è§£ææ–¹å¼
            suffix = file_path_obj.suffix.lower()

            if suffix == '.pdf':
                # å¤ç”¨ç±»ä¸­å·²ç»åˆå§‹åŒ–çš„ pdf_parser
                raw_docs = self.pdf_parser.extract(str(file_path_obj))
                # å°†æ‰€æœ‰é¡µé¢çš„å†…å®¹åˆå¹¶
                content = "\n\n".join([doc.page_content for doc in raw_docs if doc.page_content])

            elif suffix == '.docx':
                print(f"DEBUG: æ­£åœ¨å°è¯•ä½¿ç”¨ python-docx è§£æ: {file_path_obj}")  # æ·»åŠ è¿™è¡Œ
                try:
                    # ä¼˜å…ˆä½¿ç”¨ python-docx ç›´æ¥è§£æï¼Œæ›´ç¨³å®š
                    doc = docx.Document(str(file_path_obj))
                    content = "\n".join([para.text for para in doc.paragraphs])
                    print(f"DEBUG: python-docx è§£ææˆåŠŸï¼Œæå–å­—ç¬¦æ•°: {len(content)}")  # æ·»åŠ è¿™è¡Œ
                except Exception as e:
                    # å›é€€åˆ° Unstructured
                    print(f"DEBUG: python-docx å¤±è´¥ï¼ŒåŸå› : {e}")  # æ·»åŠ è¿™è¡Œ
                    logger.warning(f"python-docx è§£æå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨ Unstructured: {e}")
                    loader = UnstructuredWordDocumentLoader(str(file_path_obj))
                    docs = loader.load()
                    content = "\n\n".join([doc.page_content for doc in docs])
            else:
                # é»˜è®¤å°è¯•ä½œä¸ºçº¯æ–‡æœ¬è¯»å–
                loader = TextLoader(str(file_path_obj), encoding='utf-8')
                docs = loader.load()
                content = "\n\n".join([doc.page_content for doc in docs])

            logger.info(f"æ–‡ä»¶è§£ææˆåŠŸï¼Œé•¿åº¦: {len(content)} å­—ç¬¦")
            return content

        except Exception as e:
            logger.error(f"è§£ææ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
            return f"Error parsing file: {str(e)}"

    def _process_unstructured_documents(self, raw_documents: List[Document]) -> List[Document]:
        """
        æ”¹è¿›ç‰ˆï¼šæ™ºèƒ½é€‰æ‹©æ˜¯å¦éœ€è¦äºŒæ¬¡åˆ‡å‰²
        """
        final_chunks = []

        for doc in raw_documents:
            # âœ… ä¿æŠ¤çš„å†…å®¹ï¼ˆè¡¨æ ¼/å›¾ç‰‡ï¼‰å®Œå…¨ä¸åˆ‡
            if doc.metadata.get("do_not_split", False):
                final_chunks.append(doc)
                continue

            # ğŸ“Š ç»Ÿè®¡æ–‡æœ¬ Token æ•°
            doc_tokens = len(self.tokenizer.encode(doc.page_content))

            # âœ… å¦‚æœå·²ç»åœ¨åˆç†èŒƒå›´å†…ï¼ˆ<600 tokensï¼‰ï¼Œå°±ä¸å†åˆ‡å‰²
            if doc_tokens <= 500:
                doc.metadata["section_context"] = doc.page_content[: 100]
                final_chunks.append(doc)
                continue

            # âŒ åªæœ‰å½“æ–‡æœ¬è¿‡é•¿æ—¶æ‰è¿›è¡ŒäºŒæ¬¡åˆ‡å‰²
            sub_chunks = self.text_splitter.split_documents([doc])

            # ä¸ºå­å—æ·»åŠ ä¸Šä¸‹æ–‡
            potential_title = doc.page_content.split('\n')[0][:100]
            for i, sub_chunk in enumerate(sub_chunks):
                if i > 0:
                    sub_chunk.page_content = f"ã€ä¸Šä¸‹æ–‡:  {potential_title}ã€‘\n{sub_chunk.page_content}"
                sub_chunk.metadata["section_context"] = potential_title
                final_chunks.append(sub_chunk)

        return final_chunks

    def _normalize_path(self, file_path: Any) -> str:
        """
        ç»Ÿä¸€è·¯å¾„æ ¼å¼ï¼šç»å¯¹è·¯å¾„ + ç»Ÿä¸€è½¬ä¸ºå°å†™ã€‚
        è§£å†³ Windows ä¸‹ D: å’Œ d: ä¸ä¸€è‡´å¯¼è‡´æ— æ³•å‘½ä¸­ç¼“å­˜çš„é—®é¢˜ã€‚
        """
        try:
            return str(Path(file_path).resolve()).lower()
        except Exception:
            return str(file_path).lower()

    def _calculate_file_hash(self, file_path: Path) -> str:
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                while chunk := f.read(4096):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œæ—¶å‡ºé”™ {file_path}: {e}")
            return ""

    def _generate_chunk_id(self, file_path_str: str, page_number: Any, chunk_content: str) -> str:
        """ä¸ºæ¯ä¸ªæ–‡æ¡£ç‰‡æ®µç”Ÿæˆä¸€ä¸ªç¨³å®šçš„ã€å”¯ä¸€çš„IDã€‚"""
        content_hash = hashlib.sha256(chunk_content.encode('utf-8')).hexdigest()[:16]
        unique_id_str = f"{file_path_str}#{page_number}#{content_hash}"
        return hashlib.md5(unique_id_str.encode('utf-8')).hexdigest()

    def _get_or_create_chroma(self):
        """ç¡®ä¿ Chroma å®ä¾‹å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•ä»æŒä¹…åŒ–è·¯å¾„åŠ è½½æˆ–åˆ›å»ºä¸€ä¸ªæ–°çš„ã€‚"""
        if self.vectorstore is None:
            try:
                if Path(self.chroma_persist_path).exists() and list(Path(self.chroma_persist_path).iterdir()):
                    self.vectorstore = Chroma(
                        collection_name=self.collection_name,
                        embedding_function=self.embeddings,
                        persist_directory=self.chroma_persist_path
                    )
                    try:
                        _ = self.vectorstore._collection.count()
                        logger.info(f"âœ… æˆåŠŸæ¢å¤ Chroma é›†åˆ: {self.collection_name}")
                    except Exception as e:
                        logger.warning(f"Chroma é›†åˆè®¡æ•°å¤±è´¥ï¼Œå¯èƒ½å·²æŸåæˆ–ä¸ºç©ºï¼š{e}ã€‚å°†å°è¯•åˆ›å»ºæ–°é›†åˆã€‚")
                        self.vectorstore = None
                else:
                    logger.info(f"Chroma å­˜å‚¨è·¯å¾„ '{self.chroma_persist_path}' ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºä¸€ä¸ªæ–°çš„ã€‚")
            except Exception as e:
                logger.warning(f"æ— æ³•æ¢å¤ Chroma å®ä¾‹ï¼š{e}ã€‚å°†åˆ›å»ºä¸€ä¸ªæ–°çš„ã€‚")
                self.vectorstore = None
        return self.vectorstore

    def _load_knowledge_base(self):
        """
        ä¼˜åŒ–åçš„çŸ¥è¯†åº“åŠ è½½é€»è¾‘ (v2.0 ç¨³å®šç‰ˆ)ï¼š
        1. å¼•å…¥è·¯å¾„æ ‡å‡†åŒ– (_normalize_path)ï¼Œè§£å†³ Windows ç›˜ç¬¦å¤§å°å†™é—®é¢˜ã€‚
        2. ä¼˜å…ˆæ¯”å¯¹ Hashï¼Œåªè¦ Hash æ²¡å˜ï¼Œç»å¯¹ä¸é‡æ–°è·‘ OCRã€‚
        3. å¢å¼º ChunkStore è¯»å–çš„å®¹é”™æ€§ã€‚
        """
        try:
            # 1. åŠ è½½å…ƒæ•°æ®
            metadata_file = self.kb_path / "metadata.json"
            if metadata_file.exists():
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.doc_metadata = data.get("doc_metadata", {})
                    self._file_hashes = data.get("file_hashes", {})
                logger.info(f"åŠ è½½äº† {len(self.doc_metadata)} ä¸ªæ–‡ä»¶å…ƒæ•°æ®ã€‚")
            else:
                self.doc_metadata = {}
                self._file_hashes = {}

            # 2. åˆå§‹åŒ– Chroma (å°è¯•åŠ è½½æŒä¹…åŒ–æ•°æ®)
            self._get_or_create_chroma()

            self.documents = []  # æ¸…ç©ºå†…å­˜æ–‡æ¡£åˆ—è¡¨ (ç”¨äº BM25)
            files_to_update_in_chroma = []
            files_to_remove_from_metadata = []

            # 3. éå†å…ƒæ•°æ®ä¸­çš„æ–‡ä»¶
            # ä½¿ç”¨ list() åŒ…è£… keysï¼Œé˜²æ­¢åœ¨è¿­ä»£ä¸­ä¿®æ”¹å­—å…¸æŠ¥é”™
            for stored_path_str, meta in list(self.doc_metadata.items()):
                try:
                    file_path_obj = Path(stored_path_str).resolve()
                except Exception as e:
                    logger.warning(f"è·¯å¾„è§£æå¤±è´¥: {stored_path_str}, å°†ç§»é™¤è®°å½•ã€‚")
                    files_to_remove_from_metadata.append(stored_path_str)
                    continue

                # --- æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ ---
                if not file_path_obj.exists():
                    logger.warning(f"æ–‡ä»¶ç¼ºå¤±: {file_path_obj.name}ï¼Œå°†ä»çŸ¥è¯†åº“ç§»é™¤ã€‚")
                    files_to_remove_from_metadata.append(stored_path_str)
                    continue

                # --- å…³é”®ï¼šæ ‡å‡†åŒ–è·¯å¾„ç”¨äºåç»­é€»è¾‘ ---
                normalized_path = self._normalize_path(file_path_obj)

                # --- æ ¸å¿ƒä¼˜åŒ–ï¼šåŸºäº mtime å’Œ Hash çš„å¤šçº§æ£€æŸ¥ ---
                current_mtime = str(os.path.getmtime(file_path_obj))
                last_mtime = meta.get("added_at", "0")

                file_needs_reprocess = False

                # åªæœ‰æ—¶é—´æˆ³å˜äº†ï¼Œæ‰å€¼å¾—å»ç®— Hash
                if current_mtime != last_mtime:
                    logger.info(f"æ£€æµ‹åˆ°æ—¶é—´å˜åŒ–: {file_path_obj.name}ï¼Œæ­£åœ¨æ ¡éªŒå†…å®¹å“ˆå¸Œ...")
                    current_hash = self._calculate_file_hash(file_path_obj)
                    stored_hash = self._file_hashes.get(stored_path_str)

                    if current_hash != stored_hash:
                        logger.info(f"âš ï¸ æ–‡ä»¶å†…å®¹å·²ä¿®æ”¹ (Hashä¸ä¸€è‡´): {file_path_obj.name}ï¼Œæ ‡è®°ä¸ºæ›´æ–°ã€‚")
                        file_needs_reprocess = True
                    else:
                        # å†…å®¹æ²¡å˜ï¼Œåªæ˜¯ç¢°äº†æ—¶é—´ (touch)ï¼Œæ›´æ–°å…ƒæ•°æ®ä¸­çš„æ—¶é—´å³å¯
                        logger.info(f"âœ… æ–‡ä»¶å†…å®¹æœªå˜ (Hashä¸€è‡´): {file_path_obj.name}ï¼Œä»…æ›´æ–°æ—¶é—´æˆ³ã€‚")
                        self.doc_metadata[stored_path_str]["added_at"] = current_mtime

                # --- åŠ è½½ Chunks (ç”¨äº BM25) ---
                # ã€å®¹é”™è¯»å–ã€‘ï¼šå°è¯•ç”¨åŸå§‹ Key è¯»å–ï¼Œå¦‚æœå¤±è´¥ï¼Œå°è¯•ç”¨æ ‡å‡†åŒ– Key è¯»å–
                chunks = self.chunk_store.get_chunks_for_file(stored_path_str)
                if not chunks:
                    # å°è¯•ç”¨æ ‡å‡†åŒ–è·¯å¾„è¯»å– (åº”å¯¹ Windows ç›˜ç¬¦å¤§å°å†™å˜åŒ–)
                    chunks = self.chunk_store.get_chunks_for_file(normalized_path)

                # å¦‚æœä¾ç„¶æ²¡è¯»åˆ°ï¼Œä¸”æ–‡ä»¶å†…å®¹æ²¡å˜ï¼Œè¯´æ˜æ˜¯ ChunkStore ç¼“å­˜æ–‡ä»¶æŸåæˆ–ä¸¢äº†
                if not chunks and not file_needs_reprocess:
                    logger.warning(f"âŒ ChunkStore ç¼“å­˜ç¼ºå¤±: {file_path_obj.name}ï¼Œå¿…é¡»å¼ºåˆ¶é‡æ–°è§£æã€‚")
                    file_needs_reprocess = True

                # --- åˆ†æ”¯ Aï¼šå¿«ä¹è·¯å¾„ (ç›´æ¥ä½¿ç”¨ç¼“å­˜) ---
                if not file_needs_reprocess:
                    # ç›´æ¥æŠŠç¼“å­˜çš„ chunks åŠ åˆ°å†…å­˜ç»™ BM25 ç”¨
                    # ç»å¯¹ä¸é‡æ–° OCRï¼Œä¹Ÿä¸æ“ä½œ Chroma
                    self.documents.extend(chunks)
                    continue

                # --- åˆ†æ”¯ Bï¼šæ…¢é€Ÿè·¯å¾„ (é‡æ–°è§£æ) ---
                try:
                    logger.info(f"ğŸ”„ æ­£åœ¨é‡æ–°è§£ææ–‡ä»¶: {file_path_obj.name} ...")

                    doc_type = meta.get("doc_type", "auto")

                    # 1. æ‰§è¡Œè§£æ (è€—æ—¶æ“ä½œ)
                    if doc_type == "pdf":
                        raw_docs = self.pdf_parser.extract(str(file_path_obj))
                        chunks = self._process_unstructured_documents(raw_docs)
                    elif doc_type == "text":
                        loader = TextLoader(str(file_path_obj), encoding='utf-8')
                        chunks = self.text_splitter.split_documents(loader.load())
                    elif doc_type == "docx":
                        loader = UnstructuredWordDocumentLoader(str(file_path_obj))
                        chunks = self.text_splitter.split_documents(loader.load())
                    else:
                        # å…œåº•
                        loader = TextLoader(str(file_path_obj), encoding='utf-8')
                        chunks = self.text_splitter.split_documents(loader.load())

                    # 2. é‡æ–°ç”Ÿæˆ ID å¹¶ä¿å­˜åˆ° Store
                    chunk_ids = []
                    for chunk in chunks:
                        # è¡¥å…¨ metadata
                        page = chunk.metadata.get('page', 0)
                        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ stored_path_str ä¿æŒä¸€è‡´æ€§
                        cid = self._generate_chunk_id(stored_path_str, page, chunk.page_content)

                        raw_image_paths = chunk.metadata.get("image_paths", [])
                        chunk.metadata.update({
                            "source": stored_path_str,
                            "chunk_id": cid,
                            "file_name": file_path_obj.name,
                            "image_paths": str(raw_image_paths)
                        })
                        chunk_ids.append(cid)

                    # 3. æ›´æ–° ChunkStore
                    # å»ºè®®ï¼šè¿™é‡Œå¦‚æœ stored_path_str å’Œ normalized_path ä¸ä¸€è‡´ï¼Œ
                    # ä¸ºäº†æœªæ¥ç¨³å®šæ€§ï¼Œå¯ä»¥è€ƒè™‘åˆ é™¤æ—§ keyï¼Œå­˜å…¥æ–° keyã€‚
                    # ä½†ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬è¿˜æ˜¯å­˜å…¥ stored_path_str
                    self.chunk_store.add_chunks(stored_path_str, chunks)

                    # 4. æ›´æ–°å…ƒæ•°æ®
                    self.doc_metadata[stored_path_str].update({
                        "chunk_ids": chunk_ids,
                        "chunk_count": len(chunks),
                        "added_at": current_mtime
                    })
                    self._file_hashes[stored_path_str] = self._calculate_file_hash(file_path_obj)

                    # 5. æ”¶é›†åˆ°åˆ—è¡¨ï¼Œç¨åæ›´æ–° Chroma
                    files_to_update_in_chroma.append(
                        (stored_path_str, chunks, chunk_ids)
                    )

                    # 6. åŠ å…¥å†…å­˜ (BM25)
                    self.documents.extend(chunks)

                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {stored_path_str}: {e}", exc_info=True)
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä¸åº”è¯¥ä¿ç•™é”™è¯¯çš„å…ƒæ•°æ®ï¼Œä¸‹æ¬¡åº”è¯¥é‡è¯•
                    files_to_remove_from_metadata.append(stored_path_str)

            # 4. æ‰§è¡Œæ¸…ç†å’Œ Chroma æ›´æ–°

            # A. ç§»é™¤å¤±æ•ˆæ–‡ä»¶
            kb_has_changes = False
            if files_to_remove_from_metadata or files_to_update_in_chroma:
                kb_has_changes = True

            # A. ç§»é™¤å¤±æ•ˆæ–‡ä»¶
            for fp in files_to_remove_from_metadata:
                if fp in self.doc_metadata: del self.doc_metadata[fp]
                if fp in self._file_hashes: del self._file_hashes[fp]
                self._delete_file_from_vectorstore(fp)

            # B. å¢é‡æ›´æ–° Chroma
            if files_to_update_in_chroma:
                self._get_or_create_chroma()
                if self.vectorstore:
                    for fp, chunks, cids in files_to_update_in_chroma:
                        logger.info(f"æ­£åœ¨æ›´æ–° Chroma ç´¢å¼•: {Path(fp).name} (æ¸…ç†æ—§è®°å½• -> æ·»åŠ æ–°è®°å½•)")
                        self._delete_file_from_vectorstore(fp)
                        if chunks:
                            self.vectorstore.add_documents(chunks, ids=cids)

            # 5. ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
            self._save_knowledge_base()
            logger.info(f"çŸ¥è¯†åº“åŠ è½½å®Œæˆã€‚å†…å­˜ä¸­æ–‡æ¡£ç‰‡æ®µæ•°: {len(self.documents)}")

        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†åº“æ—¶å‘ç”Ÿæœªæ•è·å¼‚å¸¸: {e}", exc_info=True)
            self.documents = []
            kb_has_changes = True  # å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œå®‰å…¨èµ·è§æ ‡è®°ä¸ºéœ€è¦é‡å»º
        finally:
            # ã€ä¿®æ”¹ç‚¹ã€‘æ ¹æ®æ£€æµ‹åˆ°çš„å˜æ›´çŠ¶æ€ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
            if 'kb_has_changes' not in locals():
                kb_has_changes = True  # å…œåº•

            self._initialize_retrievers(force_rebuild=kb_has_changes)

    def _migrate_old_data(self):
        """è¿ç§»æ—§ç‰ˆæœ¬æ•°æ®åˆ°æ–°çš„ chunk store æ ¼å¼"""
        for file_path_str, meta in list(self.doc_metadata.items()):
            file_path_obj = Path(file_path_str)

            if not file_path_obj.exists():
                logger.warning(f"æ–‡ä»¶ {file_path_obj.name} ä¸å­˜åœ¨ï¼Œè·³è¿‡è¿ç§»")
                continue

            try:
                # æ ¹æ®æ–‡æ¡£ç±»å‹è·å–åŠ è½½å™¨
                doc_type = meta.get("doc_type")
                if doc_type == "pdf":
                    loader = PyPDFLoader(str(file_path_obj))
                elif doc_type == "text":
                    loader = TextLoader(str(file_path_obj), encoding='utf-8')
                elif doc_type == "docx":
                    loader = UnstructuredWordDocumentLoader(str(file_path_obj))
                else:
                    continue

                # åŠ è½½å¹¶åˆ†å‰²
                raw_documents = loader.load()
                chunks = self.text_splitter.split_documents(raw_documents)

                # æ·»åŠ å…ƒæ•°æ®
                for chunk in chunks:
                    page_number = chunk.metadata.get('page', -1) + 1 if 'page' in chunk.metadata else 'N/A'
                    chunk.metadata.update({
                        "source": file_path_str,
                        "doc_type": doc_type,
                        "file_name": file_path_obj.name,
                        "page_number": page_number
                    })

                # å­˜å‚¨åˆ° chunk store
                self.chunk_store.add_chunks(file_path_str, chunks)
                self.documents.extend(chunks)

                logger.info(f"å·²è¿ç§»æ–‡æ¡£: {file_path_obj.name} ({len(chunks)} chunks)")

            except Exception as e:
                logger.error(f"è¿ç§»æ–‡æ¡£ {file_path_obj.name} å¤±è´¥: {e}")

        logger.info("æ•°æ®è¿ç§»å®Œæˆ")

    def _save_knowledge_base(self):
        try:
            metadata_file = self.kb_path / "metadata.json"
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "doc_metadata": self.doc_metadata,
                    "file_hashes": self._file_hashes
                }, f, ensure_ascii=False, indent=2)
            logger.info("æ–‡æ¡£å…ƒæ•°æ®å’Œæ–‡ä»¶å“ˆå¸Œä¿å­˜æˆåŠŸã€‚")

        except Exception as e:
            logger.error(f"ä¿å­˜çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}", exc_info=True)

    def _initialize_retrievers(self, force_rebuild: bool = True):
        bm25_cache_path = self.kb_path / "bm25_retriever.pkl"
        loaded_from_cache = False
        # 1. å°è¯•ä»ç¼“å­˜åŠ è½½ (å½“ä¸éœ€è¦å¼ºåˆ¶é‡å»ºä¸”ç¼“å­˜æ–‡ä»¶å­˜åœ¨æ—¶)
        if not force_rebuild and bm25_cache_path.exists():
            try:
                with open(bm25_cache_path, 'rb') as f:
                    self._bm25_retriever = pickle.load(f)
                logger.info(f"ğŸš€ æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½ BM25 ç´¢å¼•: {bm25_cache_path}")
                loaded_from_cache = True
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½ BM25 ç¼“å­˜å¤±è´¥: {e}ï¼Œå°†é‡æ–°æ„å»ºã€‚")

        # 2. å¦‚æœæœªä»ç¼“å­˜åŠ è½½ï¼Œåˆ™é‡æ–°æ„å»º
        if not loaded_from_cache:
            if self.documents:
                # ã€ä¼˜åŒ–ã€‘ä½¿ç”¨æ‚¨å®šä¹‰çš„ ChineseBM25Retriever è¿›è¡Œæ›´å¥½çš„ä¸­æ–‡åˆ†è¯
                # å¦‚æœæ‚¨æƒ³ç”¨å›æ™®é€šçš„ BM25Retrieverï¼Œè¯·æ”¹å› BM25Retriever.from_documents
                try:
                    self._bm25_retriever = ChineseBM25Retriever.from_documents(self.documents)
                except NameError:
                    # ä»¥æ­¤é˜²å¤‡ChineseBM25Retrieveræœªå®šä¹‰çš„æƒ…å†µ
                    self._bm25_retriever = BM25Retriever.from_documents(self.documents)

                logger.info(f"æˆåŠŸåˆå§‹åŒ– BM25 Retriever, åŒ…å« {len(self.documents)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")

                # æ„å»ºå®Œæˆåï¼Œä¿å­˜åˆ°ç¼“å­˜
                try:
                    with open(bm25_cache_path, 'wb') as f:
                        pickle.dump(self._bm25_retriever, f)
                    logger.info("ğŸ’¾ BM25 ç´¢å¼•å·²ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜ã€‚")
                except Exception as e:
                    logger.error(f"ä¿å­˜ BM25 ç¼“å­˜å¤±è´¥: {e}")
            else:
                self._bm25_retriever = None
                logger.warning("æ²¡æœ‰æ–‡æ¡£ç‰‡æ®µ (self.documents ä¸ºç©º), BM25 Retriever æœªåˆå§‹åŒ–ã€‚")
                # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œæ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§ç¼“å­˜ï¼Œé˜²æ­¢ä¸ä¸€è‡´
                if bm25_cache_path.exists():
                    try:
                        os.remove(bm25_cache_path)
                    except OSError:
                        pass

        self._get_or_create_chroma()
        if self.vectorstore:
            try:
                # æ˜¾å¼æŒ‡å®š k å‚æ•°ï¼Œå³ä½¿åœ¨åˆå§‹åŒ–æ£€ç´¢å™¨æ—¶ï¼Œä¹Ÿå¯ä»¥è®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
                self._chroma_retriever = self.vectorstore.as_retriever(search_kwargs={"k": 8})
                logger.info("æˆåŠŸåˆå§‹åŒ– Chroma Retrieverã€‚")
            except Exception as e:
                logger.warning(f"åˆå§‹åŒ– Chroma Retriever å¤±è´¥: {e}", exc_info=True)
                self._chroma_retriever = None
        else:
            self._chroma_retriever = None
            logger.warning("Chroma å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–, æ— æ³•åˆ›å»ºå‘é‡æ£€ç´¢å™¨ã€‚")

        if self._bm25_retriever and self._chroma_retriever:
            logger.info("æ··åˆæ£€ç´¢å‡†å¤‡å°±ç»ª: å°†åœ¨ search() ä¸­ä½¿ç”¨ RRF èåˆ BM25 å’Œ Vector ç»“æœã€‚")
        elif self._bm25_retriever or self._chroma_retriever:
            logger.info("ä»…éƒ¨åˆ†æ£€ç´¢å™¨å¯ç”¨ï¼Œsearch() å°†é™çº§ä¸ºå•è·¯æ£€ç´¢ã€‚")
        else:
            logger.error("æœªèƒ½åˆå§‹åŒ–ä»»ä½•æ£€ç´¢å™¨ã€‚æŸ¥è¯¢åŠŸèƒ½å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")

    def _rrf_fusion(self, results_list: List[List[Document]], k: int = 60) -> List[Document]:
        """
        å®ç° RRF (Reciprocal Rank Fusion) ç®—æ³•ã€‚
        å…¬å¼: score = sum(1 / (k + rank))
        k é€šå¸¸å– 60ã€‚
        """
        fused_scores = defaultdict(float)
        doc_map = {}

        for results in results_list:
            for rank, doc in enumerate(results):
                # ç”Ÿæˆå”¯ä¸€é”®ï¼šä¼˜å…ˆä½¿ç”¨ chunk_idï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨å†…å®¹å“ˆå¸Œ
                doc_id = doc.metadata.get("chunk_id")
                if not doc_id:
                    unique_str = f"{doc.metadata.get('source', '')}#{doc.metadata.get('page_number', '')}#{doc.page_content}"
                    doc_id = hashlib.md5(unique_str.encode('utf-8')).hexdigest()

                if doc_id not in doc_map:
                    doc_map[doc_id] = doc

                # RRF æ ¸å¿ƒå…¬å¼
                fused_scores[doc_id] += 1.0 / (k + rank + 1)

        # æ ¹æ®åˆ†æ•°é™åºæ’åº
        sorted_doc_ids = sorted(fused_scores.keys(), key=lambda x: fused_scores[x], reverse=True)

        # è¿”å›æ’åºåçš„æ–‡æ¡£åˆ—è¡¨
        return [doc_map[doc_id] for doc_id in sorted_doc_ids]

    def _delete_file_from_vectorstore(self, file_path_str: str):
        self._get_or_create_chroma()
        if not self.vectorstore:
            logger.warning("å‘é‡å­˜å‚¨æœªåˆå§‹åŒ–ï¼Œè·³è¿‡åˆ é™¤ Chroma è®°å½•ã€‚")
            return

        chunk_ids_to_delete = []
        if file_path_str in self.doc_metadata:
            chunk_ids_to_delete = self.doc_metadata[file_path_str].get("chunk_ids", [])

        if chunk_ids_to_delete:
            try:
                self.vectorstore.delete(ids=chunk_ids_to_delete)
                logger.info(
                    f"ä» Chroma æˆåŠŸåˆ é™¤ä¸ '{file_path_str}' ç›¸å…³çš„ {len(chunk_ids_to_delete)} æ¡è®°å½• (é€šè¿‡ chunk_ids)ã€‚")
            except Exception as e:
                logger.error(f"ä» Chroma åˆ é™¤æ–‡æ¡£ '{file_path_str}' (ä½¿ç”¨ chunk_ids) æ—¶å‡ºé”™: {e}", exc_info=True)
        else:
            logger.info(
                f"æ–‡ä»¶ '{file_path_str}' æ²¡æœ‰å…³è”çš„ chunk_ids æˆ–æœªåœ¨å…ƒæ•°æ®ä¸­æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡æºæ–‡ä»¶åä» Chroma ä¸­åˆ é™¤æ‰€æœ‰ç›¸å…³ç‰‡æ®µã€‚")

        try:
            self.vectorstore.delete(where={"source": file_path_str})
            logger.info(f"ä» Chroma æˆåŠŸåˆ é™¤ä¸ '{file_path_str}' ç›¸å…³çš„è®°å½• (é€šè¿‡ metadata è¿‡æ»¤)ã€‚")
        except Exception as e:
            logger.error(f"ä» Chroma åˆ é™¤æ–‡æ¡£ '{file_path_str}' (ä½¿ç”¨ metadata è¿‡æ»¤) æ—¶å‡ºé”™: {e}", exc_info=True)

    def add_document(self, file_path: str, doc_type: str = "auto"):
        file_path_obj = Path(file_path).resolve()
        if not file_path_obj.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path_obj}")

        # ã€ä¿®æ”¹ç‚¹ã€‘ä½¿ç”¨æ ‡å‡†åŒ–è·¯å¾„ä½œä¸ºå”¯ä¸€æ ‡è¯† Key
        file_path_str = self._normalize_path(file_path_obj)

        # è®¡ç®—å“ˆå¸Œç”¨äºæ¯”å¯¹å†…å®¹æ˜¯å¦å˜åŒ–
        current_file_hash = self._calculate_file_hash(file_path_obj)

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”æœªå˜åŒ–
        if (file_path_str in self.doc_metadata and
                self._file_hashes.get(file_path_str) == current_file_hash and
                self.chunk_store.has_file(file_path_str)):
            logger.info(f"æ–‡æ¡£ '{file_path_obj.name}' å·²å­˜åœ¨ä¸”æœªå˜åŒ–ï¼Œè·³è¿‡")
            return

        # å¦‚æœæ–‡ä»¶å­˜åœ¨ä½†å†…å®¹å˜äº†ï¼Œå…ˆæ¸…ç†æ—§æ•°æ®
        if file_path_str in self.doc_metadata:
            logger.info(f"æ–‡æ¡£ '{file_path_obj.name}' å†…å®¹å·²æ›´æ–°ï¼Œé‡æ–°å¤„ç†")
            self._delete_file_from_vectorstore(file_path_str)
            self.chunk_store.delete_file(file_path_str)  # ä» chunk store åˆ é™¤
            self.documents = [doc for doc in self.documents if doc.metadata.get('source') != file_path_str]

        logger.info(f"æ­£åœ¨æ·»åŠ æ–‡æ¡£: {file_path_obj}")

        try:
            # 1. åŠ è½½æ–‡æ¡£ (è§£æ)
            if doc_type == "pdf" or (doc_type == "auto" and file_path_obj.suffix.lower() == '.pdf'):
                doc_type = "pdf"
                raw_documents = self.pdf_parser.extract(file_path_str)
                # ã€ä¿®æ”¹ç‚¹ã€‘é’ˆå¯¹ PDF ä½¿ç”¨è‡ªå®šä¹‰çš„æ··åˆåˆ‡å‰²é€»è¾‘
                chunks = self._process_unstructured_documents(raw_documents)
            else:
                if doc_type == "auto":
                    if file_path_obj.suffix.lower() in ['.txt', '.md', '.rtf']:
                        doc_type = "text"
                        loader = TextLoader(str(file_path_obj), encoding='utf-8')
                    elif file_path_obj.suffix.lower() == '.docx':
                        doc_type = "docx"
                        loader = UnstructuredWordDocumentLoader(str(file_path_obj))
                    else:
                        logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path_obj.suffix}")
                        return
                    raw_documents = loader.load()
                chunks = self.text_splitter.split_documents(raw_documents)

            # 3. ç©ºæ–‡æ¡£æ£€æŸ¥
            if not chunks:
                logger.warning(f"æ–‡æ¡£ {file_path_obj.name} æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æœ¬ç‰‡æ®µ,è·³è¿‡ã€‚")
                if file_path_str in self.doc_metadata:
                    del self.doc_metadata[file_path_str]
                    del self._file_hashes[file_path_str]
                    self._save_knowledge_base()
                    self._initialize_retrievers()
                return

            # 4. éå†å¤„ç† Metadata å’Œç”Ÿæˆ ID
            chunk_ids_for_this_file = []
            for i, chunk in enumerate(chunks):
                # å…¼å®¹ PDFParser æˆ–å…¶ä»– Loader çš„ page å­—æ®µ
                if 'page' in chunk.metadata:
                    page_number = chunk.metadata['page'] + 1
                else:
                    page_number = 'N/A'

                # ç”Ÿæˆå”¯ä¸€ ID
                chunk_id = self._generate_chunk_id(file_path_str, page_number, chunk.page_content)
                chunk_ids_for_this_file.append(chunk_id)

                # è·å–åŸå§‹çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
                raw_image_paths = chunk.metadata.get("image_paths", [])

                # æ›´æ–° metadata (åŸåœ°ä¿®æ”¹)
                chunk.metadata.update({
                    "source": file_path_str,
                    "doc_type": doc_type,
                    "file_name": file_path_obj.name,
                    "page_number": page_number,
                    "chunk_id": chunk_id,
                    # ã€ä¿®å¤ç‚¹ã€‘Chroma ä¸æ”¯æŒåˆ—è¡¨ç±»å‹å…ƒæ•°æ®ï¼Œå¿…é¡»å°†å…¶è½¬ä¸ºå­—ç¬¦ä¸²
                    "image_paths": str(raw_image_paths)
                })

            # =======================================================
            # å…³é”®ä¿®æ”¹ï¼šä»¥ä¸‹æ‰€æœ‰å­˜å‚¨æ“ä½œå¿…é¡»åœ¨ for å¾ªç¯å¤–éƒ¨æ‰§è¡Œ
            # =======================================================

            # 5. ä¿å­˜åˆ° ChunkStore
            self.chunk_store.add_chunks(file_path_str, chunks)

            # 6. ä¿å­˜åˆ° Chroma å‘é‡æ•°æ®åº“
            self._get_or_create_chroma()
            if self.vectorstore is None:
                self.vectorstore = Chroma.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    persist_directory=self.chroma_persist_path,
                    collection_name=self.collection_name,
                    ids=chunk_ids_for_this_file
                )
                logger.info(
                    f"åˆ›å»ºäº†æ–°çš„ Chroma é›†åˆ '{self.collection_name}' å¹¶æŒä¹…åŒ–åˆ° {self.chroma_persist_path}ã€‚")
            else:
                # æ­¤æ—¶ chunks å’Œ chunk_ids_for_this_file é•¿åº¦å®Œå…¨ä¸€è‡´
                self.vectorstore.add_documents(chunks, ids=chunk_ids_for_this_file)
                logger.info(f"å‘ Chroma é›†åˆè¿½åŠ äº† {len(chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µã€‚")

            # 7. æ›´æ–°å†…å­˜æ–‡æ¡£åˆ—è¡¨ (ç”¨äº BM25)
            self.documents.extend(chunks)

            # 8. æ›´æ–°å…¨å±€å…ƒæ•°æ®
            self.doc_metadata[file_path_str] = {
                "chunk_count": len(chunks),
                "doc_type": doc_type,
                "added_at": str(os.path.getmtime(file_path_obj)),
                "chunk_ids": chunk_ids_for_this_file
            }
            self._file_hashes[file_path_str] = current_file_hash

            # 9. æŒä¹…åŒ–å…ƒæ•°æ®å¹¶é‡æ–°åˆå§‹åŒ–æ£€ç´¢å™¨
            self._save_knowledge_base()
            self._initialize_retrievers()

            logger.info(f"æˆåŠŸæ·»åŠ æ–‡æ¡£: {file_path_obj} (åˆ†å‰²ä¸º {len(chunks)} ä¸ªç‰‡æ®µ)")

        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
            # å‘ç”Ÿå¼‚å¸¸æ—¶è¿›è¡Œæ¸…ç†
            if file_path_str in self.doc_metadata:
                del self.doc_metadata[file_path_str]
                del self._file_hashes[file_path_str]
                self._save_knowledge_base()
                self._delete_file_from_vectorstore(file_path_str)
                self._initialize_retrievers()
            raise

    def add_documents_from_directory(self, directory_path: str):
        directory_path_obj = Path(directory_path).resolve()
        if not directory_path_obj.is_dir():
            raise NotADirectoryError(f"è·¯å¾„ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ç›®å½•: {directory_path_obj}")

        logger.info(f"å¼€å§‹ä»ç›®å½• {directory_path_obj} æ·»åŠ æ–‡æ¡£...")
        processed_count = 0
        all_new_chunks_for_store: List[Document] = []
        all_new_chunk_ids_for_store: List[str] = []
        processed_file_paths_metadata_temp: Dict[str, Any] = {}
        processed_file_hashes_new: Dict[str, str] = {}
        # æ³¨æ„ï¼šè¿™é‡Œ doc_type åº”è¯¥åœ¨å¾ªç¯å†…æ ¹æ®æ–‡ä»¶åˆ¤æ–­ï¼Œæˆ–è€…æ˜¯å‚æ•°ä¼ å…¥ã€‚
        # ä½ çš„åŸä»£ç åœ¨å¾ªç¯å¤–å®šä¹‰äº† doc_type="auto"ï¼Œè¿™å¯èƒ½å¯¼è‡´åç»­æ··ä¹±ï¼Œå»ºè®®ä¿ç•™é»˜è®¤è¡Œä¸ºã€‚
        default_doc_type = "auto"

        files_to_delete_from_chroma_sources = []
        files_to_delete_from_memory_paths = []

        # --- é¢„å¤„ç†ï¼šæ£€æŸ¥å“ªäº›æ–‡ä»¶è¢«åˆ é™¤äº† ---
        current_known_files_in_dir = set()
        for root, _, files in os.walk(directory_path_obj):
            for file_name in files:
                file_path = Path(root) / file_name
                # ã€ä¿®æ”¹ç‚¹ 1ã€‘ä½¿ç”¨ç»Ÿä¸€çš„è·¯å¾„æ ‡å‡†åŒ–æ–¹æ³•ï¼Œé˜²æ­¢å¤§å°å†™é—®é¢˜
                file_path_str = self._normalize_path(file_path)
                current_known_files_in_dir.add(file_path_str)

        # æ£€æŸ¥å…ƒæ•°æ®ä¸­å±äºè¯¥ç›®å½•ä½†ç°åœ¨ä¸å­˜åœ¨çš„æ–‡ä»¶
        for file_path_str in list(self.doc_metadata.keys()):
            # ç®€å•çš„åˆ¤æ–­ï¼šå¦‚æœè·¯å¾„ä»¥ç›®å½•å¼€å¤´ï¼Œä¸”ä¸åœ¨å½“å‰æ‰«æåˆ—è¡¨ä¸­
            if file_path_str.startswith(
                    self._normalize_path(directory_path_obj)) and file_path_str not in current_known_files_in_dir:
                logger.info(f"ç›®å½•ä¸­ä¸å†å­˜åœ¨æ–‡ä»¶ '{Path(file_path_str).name}', å°†ä»çŸ¥è¯†åº“ä¸­ç§»é™¤ã€‚")
                files_to_delete_from_chroma_sources.append(file_path_str)
                files_to_delete_from_memory_paths.append(file_path_str)

        # --- ä¸»å¾ªç¯ï¼šå¤„ç†æ–°å¢æˆ–ä¿®æ”¹çš„æ–‡ä»¶ ---
        for root, _, files in os.walk(directory_path_obj):
            for file_name in files:
                file_path = Path(root) / file_name

                # ã€ä¿®æ”¹ç‚¹ 1ã€‘å†æ¬¡ç¡®ä¿è¿™é‡Œä½¿ç”¨ normalize_pathï¼Œä¸ä¸Šæ–¹å’Œ chunk_store ä¿æŒä¸€è‡´
                file_path_str = self._normalize_path(file_path)

                current_file_hash = self._calculate_file_hash(file_path)

                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¸”æœªä¿®æ”¹
                if file_path_str in self.doc_metadata and self._file_hashes.get(file_path_str) == current_file_hash:
                    # å³ä½¿æ–‡ä»¶æ²¡å˜ï¼Œä¹Ÿè¦ç¡®ä¿ chunk_store é‡Œæœ‰å®ƒ (é˜²æ­¢ä¹‹å‰åªå­˜äº†å…ƒæ•°æ®æ²¡å­˜chunksçš„æƒ…å†µ)
                    if self.chunk_store.has_file(file_path_str):
                        logger.info(f"æ–‡æ¡£ '{file_path.name}' å·²å­˜åœ¨ä¸”å†…å®¹æœªå˜,è·³è¿‡å¤„ç†ã€‚")
                        processed_file_paths_metadata_temp[file_path_str] = self.doc_metadata[file_path_str]
                        processed_file_hashes_new[file_path_str] = self._file_hashes[file_path_str]
                        continue
                    else:
                        logger.warning(f"æ–‡æ¡£ '{file_path.name}' å…ƒæ•°æ®å­˜åœ¨ä½† ChunkStore ç¼ºå¤±ï¼Œå°†é‡æ–°å¤„ç†ã€‚")

                if file_path_str in self.doc_metadata:
                    logger.info(f"æ–‡æ¡£ '{file_path.name}' å†…å®¹å·²æ›´æ–°(æˆ–ç¼“å­˜ç¼ºå¤±), æ­£åœ¨é‡æ–°å¤„ç†ã€‚")
                    files_to_delete_from_chroma_sources.append(file_path_str)
                    files_to_delete_from_memory_paths.append(file_path_str)

                try:
                    # é‡ç½® doc_type ä¸º autoï¼Œæ ¹æ®æ¯ä¸ªæ–‡ä»¶åç¼€åˆ¤æ–­
                    current_doc_type = default_doc_type

                    # ã€é€»è¾‘ä¿æŒä¸å˜ã€‘è§£ææ–‡æ¡£
                    if current_doc_type == "pdf" or (current_doc_type == "auto" and file_path.suffix.lower() == '.pdf'):
                        current_doc_type = "pdf"
                        raw_documents = self.pdf_parser.extract(file_path_str)  # è¿™é‡Œä¼ å…¥è·¯å¾„å­—ç¬¦ä¸²
                        chunks = self._process_unstructured_documents(raw_documents)
                    else:
                        if current_doc_type == "auto":
                            if file_path.suffix.lower() in ['.txt', '.md', '.rtf']:
                                current_doc_type = "text"
                                loader = TextLoader(str(file_path), encoding='utf-8')
                            elif file_path.suffix.lower() == '.docx':
                                current_doc_type = "docx"
                                loader = UnstructuredWordDocumentLoader(str(file_path))
                            else:
                                logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}")
                                continue  # ä½¿ç”¨ continue è€Œä¸æ˜¯ returnï¼Œé˜²æ­¢æ‰“æ–­æ•´ä¸ªå¾ªç¯
                            raw_documents = loader.load()
                        chunks = self.text_splitter.split_documents(raw_documents)

                    if not chunks:
                        logger.warning(f"æ–‡æ¡£ {file_path.name} æ²¡æœ‰ç”Ÿæˆä»»ä½•æ–‡æœ¬ç‰‡æ®µ,è·³è¿‡ã€‚")
                        continue

                    chunk_ids_for_this_file = []
                    for i, chunk in enumerate(chunks):
                        if 'page' in chunk.metadata:
                            page_number = chunk.metadata['page'] + 1
                        else:
                            page_number = 'N/A'

                        chunk_id = self._generate_chunk_id(file_path_str, page_number, chunk.page_content)
                        chunk_ids_for_this_file.append(chunk_id)

                        raw_image_paths = chunk.metadata.get("image_paths", [])
                        chunk.metadata.update({
                            "source": file_path_str,
                            "doc_type": current_doc_type,
                            "file_name": file_path.name,
                            "page_number": page_number,
                            "chunk_id": chunk_id,
                            "image_paths": str(raw_image_paths)
                        })

                    # =======================================================
                    # ã€ä¿®æ”¹ç‚¹ 2ã€‘å…³é”®ä¿®å¤ï¼šå¿…é¡»å°† chunks ä¿å­˜åˆ° ChunkStore ç£ç›˜ç¼“å­˜ï¼
                    # =======================================================
                    self.chunk_store.add_chunks(file_path_str, chunks)
                    # =======================================================

                    all_new_chunks_for_store.extend(chunks)
                    all_new_chunk_ids_for_store.extend(chunk_ids_for_this_file)
                    processed_count += 1

                    # è®°å½•å…ƒæ•°æ®ï¼Œç¨åç»Ÿä¸€æ›´æ–°
                    processed_file_paths_metadata_temp[file_path_str] = {
                        "chunk_count": len(chunks),
                        "doc_type": current_doc_type,
                        "added_at": str(os.path.getmtime(file_path)),
                        "chunk_ids": chunk_ids_for_this_file
                    }
                    processed_file_hashes_new[file_path_str] = current_file_hash

                    logger.info(f"å·²å¤„ç†æ–‡æ¡£: {file_path.name}, ç¼“å­˜å·²ä¿å­˜ï¼Œå¾…æ·»åŠ åˆ°å‘é‡å­˜å‚¨ã€‚")

                except Exception as e:
                    logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}", exc_info=True)
                    if file_path_str in self.doc_metadata:
                        files_to_delete_from_chroma_sources.append(file_path_str)
                        files_to_delete_from_memory_paths.append(file_path_str)

        # --- åç»­æ¸…ç†å’Œå‘é‡åº“æ›´æ–°é€»è¾‘ä¿æŒä¸å˜ ---

        self._get_or_create_chroma()
        # 1. ä» Chroma åˆ é™¤æ—§æ•°æ®
        if files_to_delete_from_chroma_sources and self.vectorstore:
            for file_path_to_delete in set(files_to_delete_from_chroma_sources):
                try:
                    self.vectorstore.delete(where={"source": file_path_to_delete})
                except Exception as e:
                    logger.error(f"ä» Chroma åˆ é™¤å‡ºé”™: {e}")

        # 2. ä»å†…å­˜å’Œå…ƒæ•°æ®ä¸­æ¸…ç†
        if files_to_delete_from_memory_paths:
            self.documents = [doc for doc in self.documents if
                              doc.metadata.get('source') not in files_to_delete_from_memory_paths]
            for file_path_str in set(files_to_delete_from_memory_paths):
                if file_path_str in self.doc_metadata:
                    del self.doc_metadata[file_path_str]
                if file_path_str in self._file_hashes:
                    del self._file_hashes[file_path_str]
                # åŒæ—¶ä¹Ÿä» chunk_store æ¸…ç†æ—§æ–‡ä»¶
                self.chunk_store.delete_file(file_path_str)

        if not all_new_chunks_for_store:
            logger.info("æ²¡æœ‰éœ€è¦æ·»åŠ åˆ°å‘é‡å­˜å‚¨çš„æ–°æ–‡æ¡£ã€‚")
            self._save_knowledge_base()
            self._initialize_retrievers()
            return

        # 3. æ‰¹é‡æ·»åŠ åˆ° Chroma
        BATCH_SIZE = 5024
        total_chunks = len(all_new_chunks_for_store)

        try:
            self._get_or_create_chroma()
            if self.vectorstore is None:
                logger.info(f"Chroma å‘é‡å­˜å‚¨é¦–æ¬¡åˆ›å»ºï¼Œæ­£åœ¨æ·»åŠ  {total_chunks} ä¸ªç‰‡æ®µ...")
                self.vectorstore = Chroma.from_documents(
                    documents=all_new_chunks_for_store,
                    embedding=self.embeddings,
                    persist_directory=self.chroma_persist_path,
                    collection_name=self.collection_name,
                    ids=all_new_chunk_ids_for_store
                )
            else:
                logger.info(f"å‘å·²å­˜åœ¨ Chroma é›†åˆè¿½åŠ æ–‡æ¡£ï¼Œå…± {total_chunks} ä¸ªç‰‡æ®µã€‚")
                for i in range(0, total_chunks, BATCH_SIZE):
                    batch_chunks = all_new_chunks_for_store[i:i + BATCH_SIZE]
                    batch_ids = all_new_chunk_ids_for_store[i:i + BATCH_SIZE]
                    self.vectorstore.add_documents(batch_chunks, ids=batch_ids)

            # 4. æ›´æ–°å†…å­˜ä¸­çš„å…ƒæ•°æ®
            for file_path_str, meta in processed_file_paths_metadata_temp.items():
                self.documents.extend(
                    [doc for doc in all_new_chunks_for_store if doc.metadata.get('source') == file_path_str])
                self.doc_metadata[file_path_str] = meta
                self._file_hashes[file_path_str] = processed_file_hashes_new[file_path_str]

            self._save_knowledge_base()
            self._initialize_retrievers()

            logger.info(f"å®Œæˆä»ç›®å½• {directory_path_obj} æ·»åŠ æ–‡æ¡£ã€‚å…±å¤„ç†/æ›´æ–° {processed_count} ä¸ªæ–‡æ¡£ã€‚")
        except Exception as e:
            logger.error(f"å°†æ–‡æ¡£æ·»åŠ åˆ° Chroma æ—¶å‡ºé”™: {e}", exc_info=True)
            raise

    def _evaluate_history_relevance(self, question: str, chat_history_list: List[Any]) -> float:
        if not chat_history_list:
            return 0.0

        history_texts = []
        for msg in chat_history_list:
            if hasattr(msg, 'content'):
                history_texts.append(msg.content)
            elif isinstance(msg, dict) and 'content' in msg:
                history_texts.append(msg['content'])

        if not history_texts:
            return 0.0

        sentence_pairs = []
        for hist_text in history_texts:
            sentence_pairs.append([question, hist_text])

        try:
            scores = self.reranker.predict(sentence_pairs)
            return np.mean(scores).item()
        except Exception as e:
            logger.warning(f"è¯„ä¼°å†å²èŠå¤©ç›¸å…³æ€§æ—¶å‡ºé”™: {e}. è¿”å›é»˜è®¤ä½åˆ†æ•°ã€‚", exc_info=True)
            return 0.1

    def _rewrite_queries(self, question: str, chat_history_list: List[Any], num_queries: int = 5,
                         history_relevance_threshold: float = 0.5) -> list[str]:
        use_history_for_rewrite = False
        if chat_history_list:
            history_relevance = self._evaluate_history_relevance(question, chat_history_list)
            logger.info(f"é—®é¢˜ä¸å†å²èŠå¤©çš„ç›¸å…³æ€§åˆ†æ•° (ç”¨äºæŸ¥è¯¢é‡å†™): {history_relevance:.4f}")
            if history_relevance >= history_relevance_threshold:
                use_history_for_rewrite = True
                logger.info("é—®é¢˜ä¸å†å²èŠå¤©ç›¸å…³åº¦é«˜ï¼Œå°†åœ¨æŸ¥è¯¢é‡å†™ä¸­è€ƒè™‘å†å²å¯¹è¯ã€‚")
            else:
                logger.info("é—®é¢˜ä¸å†å²èŠå¤©ç›¸å…³åº¦ä½ï¼ŒæŸ¥è¯¢é‡å†™å°†ä»…åŸºäºå½“å‰é—®é¢˜ã€‚")
        else:
            logger.info("æ²¡æœ‰å†å²èŠå¤©è®°å½•ï¼ŒæŸ¥è¯¢é‡å†™å°†ä»…åŸºäºå½“å‰é—®é¢˜ã€‚")

        QUERY_REWRITER_PROMPT = ChatPromptTemplate.from_messages(
            [
                ("system", """ä½ æ˜¯ä¸€ä¸ªé«˜çº§æŸ¥è¯¢é‡å†™å™¨ï¼Œæ—¨åœ¨å¸®åŠ©ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿæé«˜å¬å›ç‡ã€‚
                 æ ¹æ®ç”¨æˆ·æå‡ºçš„ã€é—®é¢˜ã€‘å’Œ{history_instruction}ï¼Œç”Ÿæˆ {num_queries} æ¡è¯­ä¹‰ä¸Šç›¸å…³ä½†è¡¨è¿°ä¸åŒçš„æ£€ç´¢å­å¥ã€‚
                 è¿™äº›å­å¥åº”è¯¥ï¼š
                 1. åŒ…å«åŸå§‹é—®é¢˜çš„æ ¸å¿ƒæ„å›¾ã€‚
                 2. {history_guidance}
                 3. ä½¿ç”¨åŒä¹‰è¯ã€è¿‘ä¹‰è¯æˆ–ç›¸å…³æœ¯è¯­ã€‚
                 4. è€ƒè™‘ä¸åŒçš„å…³é”®è¯ç»„åˆã€‚
                 5. å¦‚æœé—®é¢˜æ¶‰åŠç‰¹å®šå®ä½“æˆ–æ¦‚å¿µï¼Œå¯ä»¥å°è¯•ä»ä¸åŒè§’åº¦æé—®ã€‚
                 é™¤äº†æŸ¥è¯¢å­å¥ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ï¼Œä¸è¦ç¼–å·ï¼Œä¸è¦è§£é‡Šï¼Œä¸è¦é¢å¤–è¯´æ˜ã€‚
                 è¯·ç¡®ä¿æ¯ä¸ªå­å¥éƒ½æ¸…æ™°ã€ç®€æ´ï¼Œä¸”æ—¨åœ¨æœ€å¤§åŒ–æ£€ç´¢çš„ç›¸å…³æ€§ã€‚
                 """),
                MessagesPlaceholder("chat_history"),
                ("user", "ã€é—®é¢˜ã€‘: {question}\nè¾“å‡ºï¼š")
            ]
        )

        history_instruction = "ã€å†å²å¯¹è¯ã€‘" if use_history_for_rewrite else ""
        history_guidance = "è€ƒè™‘ã€å†å²å¯¹è¯ã€‘ä¸­å¯èƒ½å­˜åœ¨çš„ä¸Šä¸‹æ–‡å’ŒæŒ‡ä»£å…³ç³»ã€‚" if use_history_for_rewrite else "å¿½ç•¥å†å²å¯¹è¯ï¼Œèšç„¦å½“å‰é—®é¢˜ã€‚"

        try:
            rewriter_input = {
                "question": question,
                "chat_history": chat_history_list if use_history_for_rewrite else [],
                "num_queries": num_queries,
                "history_instruction": history_instruction,
                "history_guidance": history_guidance
            }

            raw_queries = (
                    QUERY_REWRITER_PROMPT
                    | self.llm
                    | StrOutputParser()
            ).invoke(rewriter_input)

            lines = []
            for line in raw_queries.splitlines():
                stripped_line = line.strip()
                if stripped_line and not stripped_line.startswith("ã€") and not stripped_line.startswith(
                        "---") and not stripped_line.startswith("###"):
                    lines.append(stripped_line)

            if question not in lines:
                lines.insert(0, question)

            unique_queries = list(dict.fromkeys(lines))
            logger.info(f"ç”Ÿæˆçš„å”¯ä¸€æŸ¥è¯¢: {unique_queries[:num_queries]}")  # è®°å½•æœ€ç»ˆæŸ¥è¯¢
            return unique_queries[:num_queries]

        except Exception as e:
            logger.error(f"æŸ¥è¯¢é‡å†™å¤±è´¥: {e}. è¿”å›åŸå§‹æŸ¥è¯¢ã€‚", exc_info=True)
            return [question]

    def _multi_query_search(self, question: str, chat_history_list: List[Any], k_each: int = 8, max_total: int = 20,
                            history_relevance_threshold_for_rewrite: float = 0.5) -> List[Document]:

        queries = self._rewrite_queries(question, chat_history_list, num_queries=3,  # å‡å°‘æŸ¥è¯¢æ•°ä»¥æé«˜é€Ÿåº¦
                                        history_relevance_threshold=history_relevance_threshold_for_rewrite)
        logger.info(f"ç”¨äºæ£€ç´¢çš„æŸ¥è¯¢: {queries}")

        all_results_list = []

        for q in queries:
            # search ç°åœ¨å†…éƒ¨å·²ç»æ˜¯ (BM25 + Vector) çš„ RRF ç»“æœäº†
            docs = self.search(q, k=k_each)
            if docs:
                all_results_list.append(docs)
        if not all_results_list:
            return []
        unique_docs = self._rrf_fusion(all_results_list, k=60)
        return unique_docs[:max_total]

    def search(self, query: str, k: int = 8) -> List[Document]:

        bm25_results = []
        chroma_results = []
        # 1. è·å– BM25 ç»“æœ
        if self._bm25_retriever:
            try:
                # åŠ¨æ€è®¾ç½® k
                self._bm25_retriever.k = k
                bm25_results = self._bm25_retriever.invoke(query)
                logger.debug(f"BM25 æ£€ç´¢åˆ° {len(bm25_results)} ä¸ªæ–‡æ¡£ã€‚")
            except Exception as e:
                logger.error(f"BM25 æ£€ç´¢å‡ºé”™: {e}")

        # 2. è·å– Vector ç»“æœ
        if self._chroma_retriever:
            try:
                # åŠ¨æ€è°ƒæ•´ Vector search çš„ k
                chroma_results = self.vectorstore.similarity_search(query, k=k)
                logger.debug(f"Chroma æ£€ç´¢åˆ° {len(chroma_results)} ä¸ªæ–‡æ¡£ã€‚")
            except Exception as e:
                logger.error(f"Chroma æ£€ç´¢å‡ºé”™: {e}")

        # 3. å¦‚æœåªæœ‰ä¸€ä¸ªæ£€ç´¢å™¨å·¥ä½œï¼Œç›´æ¥è¿”å›ç»“æœ
        if not bm25_results and not chroma_results:
            return []
        if not bm25_results:
            return chroma_results
        if not chroma_results:
            return bm25_results

        # 4. æ‰§è¡Œ RRF èåˆ
        # æˆ‘ä»¬å¯ä»¥ç¨å¾®è°ƒå¤§ k å€¼ä»¥å¹³æ»‘æ’åï¼Œæˆ–è€…ä¿æŒæ ‡å‡† 60
        fused_docs = self._rrf_fusion([bm25_results, chroma_results], k=60)

        # æˆªæ–­ç»“æœï¼Œè¿”å›å‰ k ä¸ªï¼ˆæˆ–è€…ç¨å¾®å¤šä¸€ç‚¹ä¾›åç»­é‡æ’ï¼‰
        return fused_docs[:k]

    def query(self, question: str, session_id: str = "default_session", k: int = 8, rerank_top_n: int = 7,
                  history_relevance_threshold_for_llm: float = 0.5,
                  history_relevance_threshold_for_rewrite: float = 0.5,
                  max_context_length: int = 4000) -> Dict[str, Any]:

            # å†…éƒ¨å‡½æ•°ï¼šè·å–å†å²è®°å½•å¯¹è±¡
            def get_session_history(session_id: str) -> BaseChatMessageHistory:
                return SQLChatMessageHistory(session_id=session_id,
                                             connection=f"sqlite:///{self.chat_history_db_path}")

            # å†…éƒ¨å‡½æ•°ï¼šå›¾ç‰‡è½¬Base64
            def encode_image(image_path):
                try:
                    with open(image_path, "rb") as image_file:
                        return base64.b64encode(image_file.read()).decode('utf-8')
                except Exception as e:
                    logger.error(f"å›¾ç‰‡ç¼–ç å¤±è´¥ {image_path}: {e}")
                    return None


            confidence = 0.0
            relevant_docs_reranked = []  # åˆå§‹åŒ–ï¼Œé¿å…å¼‚å¸¸æ—¶å¼•ç”¨æŠ¥é”™

            try:
                # 1. è·å–å†å²è®°å½•
                current_chat_history = get_session_history(session_id)
                current_chat_history_list = current_chat_history.messages

                # 2. æ£€ç´¢é€»è¾‘ (ä¿æŒåŸé€»è¾‘ï¼šé‡å†™ -> å¤šè·¯æ£€ç´¢ -> RRF)
                relevant_docs = self._multi_query_search(
                    question,
                    current_chat_history_list,
                    k_each=k,
                    max_total=k * 2,
                    history_relevance_threshold_for_rewrite=history_relevance_threshold_for_rewrite
                )

                # 3. Rerank å’Œ æ–‡æœ¬ä¸Šä¸‹æ–‡æ„å»º
                context_parts = []

                if relevant_docs:
                    logger.info(f"æ£€ç´¢åˆ° {len(relevant_docs)} ä¸ªæ–‡æ¡£ï¼Œæ­£åœ¨è¿›è¡Œé‡æ–°æ’å...")
                    if self.reranker:
                        sentence_pairs = [[question, doc.page_content] for doc in relevant_docs]
                        rerank_scores = self.reranker.predict(sentence_pairs)

                        # ç»„åˆå¹¶æ’åº
                        doc_with_scores = sorted(zip(relevant_docs, rerank_scores), key=lambda x: x[1], reverse=True)

                        # æˆªå– Top N
                        relevant_docs_reranked = [doc for doc, score in doc_with_scores[:rerank_top_n]]

                        if relevant_docs_reranked:
                            confidence = doc_with_scores[0][1]  # æœ€é«˜åˆ†ä½œä¸ºåŸºç¡€ç½®ä¿¡åº¦

                            # æ„å»ºæ–‡æœ¬ Context
                            current_context_length = 0
                            for doc in relevant_docs_reranked:
                                # åŒ…å«æ¥æºä¿¡æ¯
                                source_info = f"\n--- Source: {doc.metadata.get('file_name', 'Unknown')}, Page: {doc.metadata.get('page_number', 'N/A')} ---\n"
                                chunk_content = doc.page_content + source_info

                                if current_context_length + len(chunk_content) <= max_context_length:
                                    context_parts.append(chunk_content)
                                    current_context_length += len(chunk_content)
                                else:
                                    remaining = max_context_length - current_context_length
                                    if remaining > len(source_info):
                                        context_parts.append(
                                            doc.page_content[:remaining - len(source_info)] + source_info)
                                    break

                            context = "\n\n".join(context_parts)
                        else:
                            context = ""
                            confidence = 0.2
                    else:
                        # æ—  Reranker çš„å›é€€é€»è¾‘
                        relevant_docs_reranked = relevant_docs[:rerank_top_n]
                        context = "\n\n".join([d.page_content for d in relevant_docs_reranked])
                        confidence = 0.3
                else:
                    context = ""
                    confidence = 0.1

                # 4. ã€æ–°å¢ã€‘æå–å¹¶å¤„ç†ç›¸å…³å›¾ç‰‡
                relevant_images = []
                seen_images = set()

                if relevant_docs_reranked:
                    for doc in relevant_docs_reranked:
                        # è·å– image_pathsï¼Œå¤„ç†å¯èƒ½çš„ç±»å‹å·®å¼‚ (list vs string)
                        img_paths_raw = doc.metadata.get("image_paths", [])
                        if isinstance(img_paths_raw, str):
                            try:
                                img_paths = ast.literal_eval(img_paths_raw)
                            except:
                                img_paths = []
                        else:
                            img_paths = img_paths_raw

                        # éªŒè¯å¹¶æ·»åŠ 
                        if img_paths:
                            for img_path in img_paths:
                                if img_path and img_path not in seen_images and Path(img_path).exists():
                                    relevant_images.append(img_path)
                                    seen_images.add(img_path)

                # é™åˆ¶å›¾ç‰‡æ•°é‡ï¼Œé˜²æ­¢ Token æ¶ˆè€—è¿‡å¤§ (ä¾‹å¦‚æœ€å¤š3å¼ )
                relevant_images = relevant_images[:3]
                if relevant_images:
                    logger.info(f"ä¸Šä¸‹æ–‡åŒ…å« {len(relevant_images)} å¼ ç›¸å…³å›¾ç‰‡ï¼Œå°†å¯ç”¨å¤šæ¨¡æ€å›ç­”ã€‚")

                # 5. æ„å»º System Prompt
                use_history_for_llm = False
                if current_chat_history_list:
                    relevance = self._evaluate_history_relevance(question, current_chat_history_list)
                    if relevance >= history_relevance_threshold_for_llm:
                        use_history_for_llm = True

                history_instruction = "ç»“åˆã€å†å²å¯¹è¯ã€‘" if use_history_for_llm else "å¿½ç•¥å†å²å¯¹è¯"

                system_prompt_text = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çŸ¥è¯†åº“åŠ©ç†ã€‚
    è¯·æ ¹æ®ä»¥ä¸‹ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘ï¼ˆå¯èƒ½åŒ…å«æ–‡æœ¬ã€è¡¨æ ¼Markdownå’Œå›¾ç‰‡ï¼‰å’Œ{history_instruction}ï¼Œå‡†ç¡®ã€å…¨é¢ã€è¯¦å°½åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

    "\nã€é‡è¦åŸåˆ™ã€‘",
                "1.  **ä¿¡æ¯é™å®šï¼š** ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼é™å®šåœ¨æä¾›çš„ã€ä¸Šä¸‹æ–‡ã€‘ä¿¡æ¯èŒƒå›´å†…ã€‚",
                "2.  **å¤„ç†ä¿¡æ¯ä¸è¶³ï¼š** å¦‚æœä¸Šä¸‹æ–‡æœªèƒ½æä¾›å®Œæ•´æˆ–ç›´æ¥çš„ç­”æ¡ˆï¼Œè¯·å¦è¯šåœ°æŒ‡å‡ºâ€œç°æœ‰ä¿¡æ¯ä¸è¶³ä»¥å®Œå…¨å›ç­”æ­¤é—®é¢˜â€æˆ–â€œåœ¨æä¾›çš„èµ„æ–™ä¸­æœªæ‰¾åˆ°ç›´æ¥ç­”æ¡ˆã€‚â€ã€‚**ä¸¥ç¦ä»»ä½•å½¢å¼çš„è‡†æµ‹æˆ–è¡¥å……å¤–éƒ¨ä¿¡æ¯ã€‚**",
                "3.  **æ•´åˆä¸æç‚¼ï¼š** å½“ä¸Šä¸‹æ–‡åŒ…å«å¤šä¸ªç›¸å…³ç‰‡æ®µæ—¶ï¼Œè¯·å°†å®ƒä»¬æ•´åˆèµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªè¿è´¯ã€æœ‰é€»è¾‘çš„å›ç­”ã€‚**å¦‚æœä¿¡æ¯è¶³å¤Ÿï¼Œè¯·å°½å¯èƒ½è¯¦ç»†åœ°å±•å¼€ã€‚**",
                # å¼ºè°ƒè¯¦ç»†å±•å¼€
                "4.  **ç»“æ„åŒ–å‘ˆç°ï¼š** å¦‚æœé—®é¢˜å¤æ‚ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨åˆ—è¡¨ã€åˆ†ç‚¹è¯´æ˜ç­‰æ–¹å¼ä½¿ç­”æ¡ˆæ›´æ˜“è¯»ã€‚",
                "5.  **è¯­è¨€é£æ ¼ï¼š** ä¿æŒä¸“ä¸šã€å®¢è§‚å’Œä¸­ç«‹çš„è¯­æ°”ã€‚"
    """

                # 6. æ„å»º User Message (å¤šæ¨¡æ€)
                content_blocks = []

                # 6.1 æ·»åŠ æ–‡æœ¬éƒ¨åˆ†
                text_payload = f"ã€ä¸Šä¸‹æ–‡ã€‘\n{context}\n\nã€ç”¨æˆ·é—®é¢˜ã€‘\n{question}\n\nã€ä½ çš„è¯¦ç»†å›ç­”ã€‘"
                content_blocks.append({"type": "text", "text": text_payload})

                # 6.2 æ·»åŠ å›¾ç‰‡éƒ¨åˆ†
                for img_path in relevant_images:
                    base64_img = encode_image(img_path)
                    if base64_img:
                        content_blocks.append({
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_img}",
                                "detail": "auto"
                            }
                        })

                # 7. ç»„è£…æœ€ç»ˆæ¶ˆæ¯åˆ—è¡¨ (System + History + User)
                final_messages = [SystemMessage(content=system_prompt_text)]

                if use_history_for_llm:
                    final_messages.extend(current_chat_history_list)

                final_messages.append(HumanMessage(content=content_blocks))

                # 8. è°ƒç”¨ LLM ç”Ÿæˆå›ç­”
                # æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ invoke LLMï¼Œç»•è¿‡äº† RunnableWithMessageHistoryï¼Œå› ä¸ºå¤šæ¨¡æ€æ¶ˆæ¯æ„é€ æ¯”è¾ƒç‰¹æ®Š
                response = self.llm.invoke(final_messages)
                answer = response.content

                # 9. æ‰‹åŠ¨ä¿å­˜å†å²è®°å½• (å­˜çº¯æ–‡æœ¬å³å¯)
                # ç”¨æˆ·é—®é¢˜å­˜æ–‡æœ¬
                current_chat_history.add_user_message(question)
                # AI å›ç­”å­˜æ–‡æœ¬
                current_chat_history.add_ai_message(answer)

                # 10. LLM è‡ªè¯„ä¼° (ä»…åŸºäºæ–‡æœ¬ä¸Šä¸‹æ–‡å’Œå›ç­”ï¼ŒèŠ‚çœå›¾ç‰‡Token)
                # è¿™é‡Œçš„é€»è¾‘ä¿æŒä¸å˜ï¼Œç”¨äºç”Ÿæˆ confidence
                llm_eval_prompt = ChatPromptTemplate.from_messages([
                ("system", """ä½ æ˜¯ä¸€ä¸ªè¯„ä¼°åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„åŸå§‹ã€é—®é¢˜ã€‘ã€ã€æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ã€‘å’Œã€AIç”Ÿæˆçš„ç­”æ¡ˆã€‘ï¼Œåˆ¤æ–­AIç­”æ¡ˆçš„è´¨é‡å’Œå¿ å®æ€§ã€‚
                            è¯·ä¸¥æ ¼æ ¹æ®ä»¥ä¸‹ä¸‰ç‚¹è¿›è¡Œè¯„ä¼°ï¼š
                            1. ç­”æ¡ˆæ˜¯å¦å®Œå…¨åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Ÿ(Faithfulness) - å¦‚æœç­”æ¡ˆåŒ…å«ä¸Šä¸‹æ–‡ä¹‹å¤–çš„ä¿¡æ¯ï¼Œåˆ™å¿ å®åº¦ä½ã€‚
                            2. ç­”æ¡ˆæ˜¯å¦ç›´æ¥ã€å®Œæ•´åœ°å›ç­”äº†é—®é¢˜ï¼Ÿ(Relevance & Completeness) - ç­”æ¡ˆæ˜¯å¦æ¶µç›–äº†é—®é¢˜çš„æ‰€æœ‰æ–¹é¢ï¼Œå¹¶ä¸”ä¸é—®é¢˜é«˜åº¦ç›¸å…³ã€‚
                            3. ç»¼åˆä¸Šè¿°ï¼Œè¯·ç»™å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•´ä½“ç½®ä¿¡åº¦åˆ†æ•°ï¼Œè¡¨ç¤ºä½ å¯¹è¿™ä¸ªAIç­”æ¡ˆçš„ä¿¡ä»»ç¨‹åº¦ã€‚

                            **é‡è¦æç¤ºï¼šè¯·åªè¾“å‡ºä¸€ä¸ªä»‹äº0.0åˆ°1.0ä¹‹é—´çš„æµ®ç‚¹æ•°ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è§£é‡Šæˆ–æ ‡ç‚¹ç¬¦å·ã€‚**
                            """),  # å¢åŠ æ›´ä¸¥æ ¼çš„æç¤º
                ("user", "ã€é—®é¢˜ã€‘: {question}\nã€ä¸Šä¸‹æ–‡ã€‘: {context}\nã€AIç”Ÿæˆçš„ç­”æ¡ˆã€‘: {answer}\næœ€ç»ˆç½®ä¿¡åº¦åˆ†æ•°:")
            ])

                eval_chain = llm_eval_prompt | self.llm | StrOutputParser()

                try:
                    # è¯„ä¼°æ—¶ä¸ä¼ å›¾ç‰‡ï¼Œåªä¼ æ–‡æœ¬ä¸Šä¸‹æ–‡
                    eval_res = eval_chain.invoke({"question": question, "context": context, "answer": answer})
                    import re
                    match = re.search(r"(\d+\.\d+)", eval_res)
                    if match:
                        llm_conf = float(match.group(1))
                        llm_conf = max(0.0, min(1.0, llm_conf))
                        # ç»¼åˆè¯„åˆ†ï¼šLLMè¯„ä¼°å 70%ï¼Œæ£€ç´¢Rerankåˆ†å 30%
                        confidence = (llm_conf * 0.7 + confidence * 0.3) if relevant_docs else llm_conf
                except Exception as e:
                    logger.warning(f"è‡ªè¯„ä¼°å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç½®ä¿¡åº¦")

                # 11. æ•´ç† Sources è¿”å›ç»“æœ
                sources = []
                seen_source_identifiers = set()

                # æ·»åŠ æ–‡æ¡£æ¥æº
                if relevant_docs_reranked:
                    # é‡æ–°æ‰¾åˆ°å¯¹åº”çš„ rerank score (å¦‚æœæœ‰çš„è¯)
                    doc_score_map = {doc.metadata.get('chunk_id'): score
                                     for doc, score in zip([d for d, s in doc_with_scores], [s for d, s in
                                                                                             doc_with_scores])} if 'doc_with_scores' in locals() else {}

                    for doc in relevant_docs_reranked:
                        file_name = doc.metadata.get('file_name', 'Unknown')
                        page_number = doc.metadata.get('page_number', 'N/A')
                        chunk_id = doc.metadata.get('chunk_id')
                        score = doc_score_map.get(chunk_id, 0.0)

                        identifier = f"{file_name}#page_{page_number}"
                        if identifier not in seen_source_identifiers:
                            sources.append({
                                "type": "text",
                                "file": file_name,
                                "page": page_number,
                                "rerank_score": score,
                                "content_preview": doc.page_content[:100] + "..."
                            })
                            seen_source_identifiers.add(identifier)

                # æ·»åŠ å›¾ç‰‡æ¥æºä¿¡æ¯
                for img_path in relevant_images:
                    sources.append({
                        "type": "image",
                        "file": Path(img_path).name,  # å›¾ç‰‡æ–‡ä»¶å
                        "page": "Image Reference",
                        "rerank_score": 1.0,  # å›¾ç‰‡ä½œä¸ºç›´æ¥è¯æ®ï¼Œç½®ä¿¡åº¦è®¾ä¸ºé«˜
                        "content_preview": f"Image Path: {img_path}"
                    })

                return {
                    "answer": answer,
                    "sources": sources,
                    "confidence": confidence,
                }

            except Exception as e:
                logger.exception(f"æŸ¥è¯¢æ—¶å‡ºé”™: {e}")
                return {
                    "answer": f"æŠ±æ­‰, å¤„ç†æ‚¨çš„æŸ¥è¯¢æ—¶å‡ºç°äº†é”™è¯¯: {str(e)}",
                    "sources": [],
                    "confidence": 0.0,
                }
    def clear_chat_history(self, session_id: Optional[str] = None):
        if session_id:
            try:
                history = SQLChatMessageHistory(session_id=session_id,
                                                connection=f"sqlite:///{self.chat_history_db_path}")
                history.clear()
                logger.info(f"ä¼šè¯ '{session_id}' çš„å¯¹è¯å†å²å·²ä»æ•°æ®åº“ä¸­æ¸…ç©ºã€‚")
            except Exception as e:
                logger.warning(f"æ¸…ç©ºä¼šè¯ '{session_id}' å†å²æ—¶å‡ºé”™: {e}", exc_info=True)
        else:
            if Path(self.chat_history_db_path).exists():
                try:
                    os.remove(self.chat_history_db_path)
                    logger.info(f"æ‰€æœ‰ä¼šè¯çš„å¯¹è¯å†å²æ•°æ®åº“æ–‡ä»¶ '{self.chat_history_db_path}' å·²åˆ é™¤ã€‚")
                    Path(self.chat_history_db_path).parent.mkdir(parents=True, exist_ok=True)
                except Exception as e:
                    logger.error(f"åˆ é™¤æ‰€æœ‰ä¼šè¯å†å²æ•°æ®åº“æ–‡ä»¶æ—¶å‡ºé”™: {e}", exc_info=True)
            else:
                logger.info("èŠå¤©å†å²æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç©ºã€‚")

    def list_documents(self) -> Dict[str, Any]:
        total_chunks_in_memory = len(self.documents)
        chroma_index_size = 0

        self._get_or_create_chroma()
        try:
            if self.vectorstore is not None:
                chroma_index_size = self.vectorstore._collection.count()
            else:
                chroma_index_size = 0
        except Exception as e:
            logger.error(f"è·å– Chroma é›†åˆå®ä½“æ•°é‡æ—¶å‡ºé”™: {e}", exc_info=True)
            chroma_index_size = -1

        return {
            "total_chunks_in_memory": total_chunks_in_memory,
            "vector_index_size": chroma_index_size,
            "documents": self.doc_metadata,
        }

    def delete_document(self, file_path: str):
        file_path_obj = Path(file_path).resolve()
        file_path_str = str(file_path_obj)

        if file_path_str in self.doc_metadata:
            logger.info(f"æ­£åœ¨åˆ é™¤æ–‡æ¡£: {file_path_str}")
            self._delete_file_from_vectorstore(file_path_str)

            self.chunk_store.delete_file(file_path_str)
            self.documents = [doc for doc in self.documents if doc.metadata.get('source') != file_path_str]

            del self.doc_metadata[file_path_str]
            del self._file_hashes[file_path_str]

            self._save_knowledge_base()
            self._initialize_retrievers()
            logger.info(f"å·²åˆ é™¤æ–‡æ¡£: {file_path_str}")
        else:
            logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨äºçŸ¥è¯†åº“å…ƒæ•°æ®ä¸­ï¼Œæ— æ³•åˆ é™¤: {file_path_str}")
            self._delete_file_from_vectorstore(file_path_str)

    def delete_all_documents(self):
        logger.info("æ­£åœ¨åˆ é™¤æ‰€æœ‰æ–‡æ¡£å¹¶æ¸…ç©ºçŸ¥è¯†åº“...")
        self.documents = []
        self.doc_metadata = {}
        self._file_hashes = {}
        self.chunk_store.clear()
        self.clear_chat_history(session_id=None)

        if Path(self.chroma_persist_path).exists():
            try:
                shutil.rmtree(self.chroma_persist_path)
                logger.info(f"å·²åˆ é™¤ Chroma æ•°æ®åº“ç›®å½•: {self.chroma_persist_path}")
                Path(self.chroma_persist_path).mkdir(parents=True, exist_ok=True)
            except Exception as e:
                logger.error(f"åˆ é™¤ Chroma æ•°æ®åº“ç›®å½•æ—¶å‡ºé”™: {e}", exc_info=True)
        else:
            logger.info("Chroma æ•°æ®åº“ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")

        self.vectorstore = None

        self._save_knowledge_base()
        self._initialize_retrievers()
        logger.info("æ‰€æœ‰æ–‡æ¡£å·²åˆ é™¤ï¼ŒçŸ¥è¯†åº“å·²æ¸…ç©ºã€‚")

    def list_all_session_ids(self) -> List[str]:
        try:
            conn_str = f"sqlite:///{self.chat_history_db_path}"
            temp_history = SQLChatMessageHistory(session_id="dummy", connection=conn_str)

            # ä» temp_history è·å– session_maker
            Session = temp_history.session_maker

            with Session() as session:
                from sqlalchemy import text
                result = session.execute(text("SELECT DISTINCT session_id FROM langchain_chat_histories")).fetchall()
                session_ids = [row[0] for row in result]
                return session_ids
        except Exception as e:
            logger.error(f"è·å–æ‰€æœ‰ä¼šè¯IDæ—¶å‡ºé”™: {e}", exc_info=True)
            return []


def main():
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        logger.error("ç¨‹åºé€€å‡º: è¯·å…ˆåœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½® OPENAI_API_KEYã€‚")
        print("è¯·åœ¨è¿è¡Œç¨‹åºå‰è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡ï¼Œä¾‹å¦‚ï¼š")
        print("export OPENAI_API_KEY='ä½ çš„APIå¯†é’¥'   ï¼ˆLinux/macOSï¼‰")
        print("set OPENAI_API_KEY=ä½ çš„APIå¯†é’¥         ï¼ˆWindows CMDï¼‰")
        print("$env:OPENAI_API_KEY='ä½ çš„APIå¯†é’¥'      ï¼ˆPowerShellï¼‰")
        return 1

    try:
        kb = MultiDocumentKnowledgeBase("./turbine_machine", openai_api_key=openai_api_key,llm_instance=None)

        current_session_id = "user1_session"

        while True:
            print("\n" + "=" * 60)
            print(f"å¤šæ–‡æ¡£çŸ¥è¯†åº“ç³»ç»Ÿ (å½“å‰ä¼šè¯ID: {current_session_id})")
            print("=" * 60)
            print("1. æ·»åŠ å•ä¸ªæ–‡æ¡£")
            print("2. ä»æ–‡ä»¶å¤¹æ·»åŠ æ‰€æœ‰æ–‡æ¡£")
            print("3. æŸ¥è¯¢é—®é¢˜ (æ”¯æŒå†å²å¯¹è¯)")
            print("4. åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£")
            print("5. åˆ é™¤æ–‡æ¡£")
            print("6. åˆ é™¤æ‰€æœ‰æ–‡æ¡£")
            print("7. æ¸…ç©ºå½“å‰ä¼šè¯å†å²")
            print("8. åˆ‡æ¢/åˆ›å»ºä¼šè¯")
            print("9. åˆ—å‡ºæ‰€æœ‰æ´»è·ƒä¼šè¯ID")
            print("10. æ¸…ç©ºæ‰€æœ‰ä¼šè¯å†å²")
            print("11. é€€å‡º")
            print("=" * 60)

            choice = input("è¯·è¾“å…¥æ‚¨çš„é€‰æ‹© (1-11): ").strip()

            if choice == '1':
                file_path = input("è¯·è¾“å…¥è¦æ·»åŠ çš„æ–‡æ¡£è·¯å¾„: ").strip()
                try:
                    kb.add_document(file_path)
                    print(f"æ–‡æ¡£ '{file_path}' æ·»åŠ æˆåŠŸã€‚")
                except FileNotFoundError:
                    print("é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ã€‚")
                except Exception as e:
                    print(f"æ·»åŠ æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")

            elif choice == '2':
                directory_path = input("è¯·è¾“å…¥è¦æ·»åŠ æ–‡æ¡£çš„æ–‡ä»¶å¤¹è·¯å¾„: ").strip()
                try:
                    kb.add_documents_from_directory(directory_path)
                    print("æ–‡ä»¶å¤¹å†…æ‰€æœ‰æ”¯æŒçš„æ–‡æ¡£å·²å¤„ç†å®Œæˆ!")
                except Exception as e:
                    print(f"å¤„ç†æ–‡ä»¶å¤¹æ—¶å¤±è´¥: {e}")

            elif choice == "3":
                question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()
                if question:
                    # å…è®¸ç”¨æˆ·è¾“å…¥æ›´å¤šå‚æ•°
                    k_val = int(input("è¯·è¾“å…¥æ¯ä¸ªæ£€ç´¢å™¨æ£€ç´¢çš„æ–‡æ¡£æ•°é‡ k (é»˜è®¤8): ") or 8)
                    rerank_top_n_val = int(input("è¯·è¾“å…¥Rerankåä¿ç•™çš„é¡¶éƒ¨æ–‡æ¡£æ•°é‡ (é»˜è®¤5): ") or 5)
                    history_relevance_threshold_for_llm = float(
                        input("è¯·è¾“å…¥LLMä½¿ç”¨å†å²å¯¹è¯çš„ç›¸å…³åº¦é˜ˆå€¼ (0.0-1.0, é»˜è®¤0.5): ") or 0.5)
                    history_relevance_threshold_for_rewrite = float(
                        input("è¯·è¾“å…¥æŸ¥è¯¢é‡å†™ä½¿ç”¨å†å²å¯¹è¯çš„ç›¸å…³åº¦é˜ˆå€¼ (0.0-1.0, é»˜è®¤0.5): ") or 0.5)
                    max_context_length_val = int(input("è¯·è¾“å…¥ä¼ é€’ç»™LLMçš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ (é»˜è®¤4000): ") or 4000)

                    print("\næ­£åœ¨æŸ¥è¯¢...")
                    result = kb.query(question, session_id=current_session_id,
                                      k=k_val,
                                      rerank_top_n=rerank_top_n_val,
                                      history_relevance_threshold_for_llm=history_relevance_threshold_for_llm,
                                      history_relevance_threshold_for_rewrite=history_relevance_threshold_for_rewrite,
                                      max_context_length=max_context_length_val)
                    print(f"\nå›ç­”: {result['answer']}")
                    print("\n" + "=" * 20 + " ç›¸å…³èµ„æ–™ " + "=" * 20)
                    if result["sources"]:
                        for source in result["sources"]:
                            print(
                                f"  - æ–‡ä»¶: {source['file']}, é¡µé¢: {source['page']}, Rerankåˆ†æ•°: {source['rerank_score']:.4f}")
                            # print(f"    å†…å®¹é¢„è§ˆ: {source['content_preview']}") # å¦‚æœéœ€è¦ï¼Œå¯ä»¥æ‰“å°é¢„è§ˆ
                    else:
                        print("  æ— ç›¸å…³èµ„æ–™ã€‚")
                    print("=" * 20 + " ç»“æŸ " + "=" * 20)
                    print(f"æœ€ç»ˆç½®ä¿¡åº¦: {result['confidence']:.4f}")

            elif choice == "4":
                documents_info = kb.list_documents()
                print("\n" + "=" * 50)
                print("çŸ¥è¯†åº“æ–‡æ¡£åˆ—è¡¨")
                print("=" * 50)
                print(f"å†…å­˜ä¸­æ–‡æ¡£ç‰‡æ®µæ€»æ•° (BM25ä½¿ç”¨): {documents_info['total_chunks_in_memory']}")
                if documents_info['vector_index_size'] == -1:
                    print(f"å‘é‡ç´¢å¼•å®ä½“æ€»æ•°: æ— æ³•ç²¾ç¡®è·å– (Chromaå¯èƒ½éœ€è¦æ‰‹åŠ¨æ£€æŸ¥æˆ–æ›´æ–°)")
                else:
                    print(f"å‘é‡ç´¢å¼•å®ä½“æ€»æ•°: {documents_info['vector_index_size']}")
                print(f"å·²æ·»åŠ åˆ°çŸ¥è¯†åº“çš„æ–‡ä»¶:")
                if documents_info["documents"]:
                    for file_path, metadata in documents_info["documents"].items():
                        print(f"  - æ–‡ä»¶: {Path(file_path).name}")
                        print(f"    è·¯å¾„: {file_path}")
                        print(f"    ç±»å‹: {metadata['doc_type']}")
                        print(f"    ç‰‡æ®µæ•°: {metadata['chunk_count']}")
                else:
                    print("  çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£ã€‚")
                print("=" * 50)

            elif choice == "5":
                file_path = input("è¯·è¾“å…¥è¦åˆ é™¤çš„æ–‡æ¡£çš„å®Œæ•´è·¯å¾„: ").strip().strip('"')
                if file_path:
                    try:
                        kb.delete_document(file_path)
                        print("æ–‡æ¡£åˆ é™¤æˆåŠŸ!")
                    except Exception as e:
                        print(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
                else:
                    print("æ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©ºã€‚")

            elif choice == "6":
                confirm = input("ç¡®å®šè¦åˆ é™¤æ‰€æœ‰æ–‡æ¡£å—ï¼Ÿæ­¤æ“ä½œä¸å¯é€†ï¼Œè¯·è¾“å…¥ 'yes' ç¡®è®¤: ").strip().lower()
                if confirm == 'yes':
                    try:
                        kb.delete_all_documents()
                        print("æ‰€æœ‰æ–‡æ¡£å·²æˆåŠŸåˆ é™¤ã€‚")
                    except Exception as e:
                        print(f"åˆ é™¤æ‰€æœ‰æ–‡æ¡£å¤±è´¥: {e}")
                else:
                    print("æ“ä½œå·²å–æ¶ˆã€‚")

            elif choice == "7":
                kb.clear_chat_history(session_id=current_session_id)
                print(f"å½“å‰ä¼šè¯ '{current_session_id}' çš„å¯¹è¯å†å²å·²æ¸…ç©ºã€‚")

            elif choice == "8":
                new_session_id = input(f"è¯·è¾“å…¥æ–°çš„ä¼šè¯ID (å½“å‰: {current_session_id}): ").strip()
                if new_session_id:
                    current_session_id = new_session_id
                    print(f"å·²åˆ‡æ¢åˆ°ä¼šè¯ID: {current_session_id}")
                else:
                    print("ä¼šè¯IDä¸èƒ½ä¸ºç©ºï¼Œæœªåˆ‡æ¢ã€‚")

            elif choice == "9":
                print("\n" + "=" * 30)
                print("æ‰€æœ‰æ´»è·ƒä¼šè¯ID (ä»æ•°æ®åº“åŠ è½½)")
                print("=" * 30)
                session_ids = kb.list_all_session_ids()
                if session_ids:
                    for sid in session_ids:
                        print(f"- {sid}")
                else:
                    print("æ— æ´»è·ƒä¼šè¯å†å²ã€‚")
                print("=" * 30)

            elif choice == "10":
                confirm = input("ç¡®å®šè¦æ¸…ç©ºæ‰€æœ‰ä¼šè¯å†å²å—ï¼Ÿæ­¤æ“ä½œä¸å¯é€†ï¼Œè¯·è¾“å…¥ 'yes' ç¡®è®¤: ").strip().lower()
                if confirm == 'yes':
                    kb.clear_chat_history(session_id=None)
                    print("æ‰€æœ‰ä¼šè¯çš„å†å²å·²æ¸…ç©ºã€‚")
                else:
                    print("æ“ä½œå·²å–æ¶ˆã€‚")

            elif choice == "11":
                print("æ„Ÿè°¢ä½¿ç”¨!")
                break

            else:
                print("æ— æ•ˆé€‰æ‹©,è¯·é‡æ–°è¾“å…¥ã€‚")

    except Exception as e:
        logger.exception(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        print(f"ç¨‹åºå‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    exit_code = main()
    if exit_code:
        exit(exit_code)

